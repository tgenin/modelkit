{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"modelkit Python framework for production ML systems. modelkit is a Python framework meant to make your ML models robust, reusable and performant in all situations you need to use them. It is meant to bridge the gap between the different uses of your algorithms. With modelkit you can ensure that the same exact code will run in production, on your machine, or on data processing pipelines. Features \u00b6 modelkit 's key features are: simple modelkit is just a Python library, use pip to install it and you are done. custom modelkit is useful whenever you need to go beyond off-the-shelf models: custom processing, heuristics, business logic, different frameworks, etc. framework agnostic you bring your own framework to the table, and you can use whatever code or library you want. Similarly, modelkit is not opinionated about how you build or train your models. organized modelkit encourages you to version and share you ML library and artifacts with others, as a Python package or as a service. Let others use and evaluate your models! fast modelkit add minimal overhead to prediction calls. Model predictions can be batched for speed (you define the batching logic). fast to code Models only need to define their prediction logic and that's it. No cumbersome pre or postprocessing logic, branching options, etc... The boilerplate code is minimal and sensible. fast to deploy Models can be served in a single CLI call using fastapi And more: composable Models can depend on other models, and evaluate them however you need to extensible Models can rely on arbitrary supporting configurations files called assets hosted on local or cloud object stores type-safe Models' inputs and outputs can be validated by pydantic , you get type annotations for your predictions and can catch errors with static type analysis tools during development. async Models support async and sync prediction functions. modelkit supports calling async code from sync code so you don't have to suffer from partially async code. testable Models carry their own unit test cases, and unit testing fixtures are available for pytest robust modelkit helps you follow software development best practices: all configurations and artifacts are explicitly versioned and tested.","title":"Home"},{"location":"#features","text":"modelkit 's key features are: simple modelkit is just a Python library, use pip to install it and you are done. custom modelkit is useful whenever you need to go beyond off-the-shelf models: custom processing, heuristics, business logic, different frameworks, etc. framework agnostic you bring your own framework to the table, and you can use whatever code or library you want. Similarly, modelkit is not opinionated about how you build or train your models. organized modelkit encourages you to version and share you ML library and artifacts with others, as a Python package or as a service. Let others use and evaluate your models! fast modelkit add minimal overhead to prediction calls. Model predictions can be batched for speed (you define the batching logic). fast to code Models only need to define their prediction logic and that's it. No cumbersome pre or postprocessing logic, branching options, etc... The boilerplate code is minimal and sensible. fast to deploy Models can be served in a single CLI call using fastapi And more: composable Models can depend on other models, and evaluate them however you need to extensible Models can rely on arbitrary supporting configurations files called assets hosted on local or cloud object stores type-safe Models' inputs and outputs can be validated by pydantic , you get type annotations for your predictions and can catch errors with static type analysis tools during development. async Models support async and sync prediction functions. modelkit supports calling async code from sync code so you don't have to suffer from partially async code. testable Models carry their own unit test cases, and unit testing fixtures are available for pytest robust modelkit helps you follow software development best practices: all configurations and artifacts are explicitly versioned and tested.","title":"Features"},{"location":"cli/","text":"modelkit CLI \u00b6 Models description \u00b6 Describe \u00b6 This CLI prints out all relevant information on a given modelkit model repository: modelkit describe [ PACKAGE ] [ --required-models ... ] Assets listing \u00b6 This CLI will show all necessary assets to run models modelkit list-assets [ PACKAGE ] [ --required-models ... ] Download necessary assets \u00b6 This CLI will download all necessary assets to run models to the current MODELKIT_ASSETS_DIR modelkit download-assets [ PACKAGE ] [ --required-models ... ] Once this is done, you can run the models without enabling a storage provider. Dependencies graph \u00b6 This CLI will create a .DOT file with a graph of all models, their assets and model dependencies. modelkit dependencies-graph [ PACKAGE ] [ --required-models ... ] This requires graphviz for the graph layout. Predictions \u00b6 batch \u00b6 This CLI will treat a given JSONL file, with one item per line and write the output of a model as another JSONL file, using multiple processes for speed modelkit batch MODEL_NAME DATA_IN DATA_OUT [ --models PACKAGE ] [ --processes N_PROCESSES ] [ --unordered ] Where: - --models to tell it where to find the model - --processes allows you to define the number of processes (defaults to all CPUs) - --unordered does not preserve the order of outputs (as with imap_unordered ), which may be faster Benchmarking \u00b6 Memory benchmark \u00b6 This CLI attempts to measure the memory consumption of a set of modelkit models: modelkit memory [ PACKAGE ] [ --required-models ... ] Time \u00b6 This CLI accepts a model and item, and will time the prediction modelkit time MODEL_NAME EXAMPLE_ITEM --models PACKAGE Serving \u00b6 modelkit provides a single CLI to run a local FastAPI server with all loaded models mounted as endpoints: modelkit serve PACKAGE [ --required-models ... ] This is useful in order to inspect the swagger. Important Note that models whose payloads are not serializable will not be exposed, this is true in particular of numpy arrays Assets management \u00b6 To list all assets: modelkit assets list To create a new asset: modelkit assets new /path/to/asset asset_category/asset_name To update an asset's minor version: modelkit assets update /path/to/asset asset_category/asset_name To push a new major version: modelkit assets update /path/to/asset asset_category/asset_name --bump-major To retrieve a single asset modelkit assets fetch asset/spec [ --download ] Use --download to force the re-download of the asset. TF serving \u00b6 To configure models from a package to be run in TF serving: modelkit tf-serving local-docker --models [ PACKAGE ] This will write a configuration file with relative paths to the model files. This is meant to be used by mounting the MODELKIT_ASSETS_DIR in the container under the path /config . Other options include: - local-process To create a config file with absolute paths to the assets under MODELKIT_ASSETS_DIR - remote which will use whichever remote paths are found for the assets (i.e. as configured by the MODELKIT_STORAGE_PROVIDER )","title":"CLI"},{"location":"cli/#modelkit-cli","text":"","title":"modelkit CLI"},{"location":"cli/#models-description","text":"","title":"Models description"},{"location":"cli/#describe","text":"This CLI prints out all relevant information on a given modelkit model repository: modelkit describe [ PACKAGE ] [ --required-models ... ]","title":"Describe"},{"location":"cli/#assets-listing","text":"This CLI will show all necessary assets to run models modelkit list-assets [ PACKAGE ] [ --required-models ... ]","title":"Assets listing"},{"location":"cli/#download-necessary-assets","text":"This CLI will download all necessary assets to run models to the current MODELKIT_ASSETS_DIR modelkit download-assets [ PACKAGE ] [ --required-models ... ] Once this is done, you can run the models without enabling a storage provider.","title":"Download necessary assets"},{"location":"cli/#dependencies-graph","text":"This CLI will create a .DOT file with a graph of all models, their assets and model dependencies. modelkit dependencies-graph [ PACKAGE ] [ --required-models ... ] This requires graphviz for the graph layout.","title":"Dependencies graph"},{"location":"cli/#predictions","text":"","title":"Predictions"},{"location":"cli/#batch","text":"This CLI will treat a given JSONL file, with one item per line and write the output of a model as another JSONL file, using multiple processes for speed modelkit batch MODEL_NAME DATA_IN DATA_OUT [ --models PACKAGE ] [ --processes N_PROCESSES ] [ --unordered ] Where: - --models to tell it where to find the model - --processes allows you to define the number of processes (defaults to all CPUs) - --unordered does not preserve the order of outputs (as with imap_unordered ), which may be faster","title":"batch"},{"location":"cli/#benchmarking","text":"","title":"Benchmarking"},{"location":"cli/#memory-benchmark","text":"This CLI attempts to measure the memory consumption of a set of modelkit models: modelkit memory [ PACKAGE ] [ --required-models ... ]","title":"Memory benchmark"},{"location":"cli/#time","text":"This CLI accepts a model and item, and will time the prediction modelkit time MODEL_NAME EXAMPLE_ITEM --models PACKAGE","title":"Time"},{"location":"cli/#serving","text":"modelkit provides a single CLI to run a local FastAPI server with all loaded models mounted as endpoints: modelkit serve PACKAGE [ --required-models ... ] This is useful in order to inspect the swagger. Important Note that models whose payloads are not serializable will not be exposed, this is true in particular of numpy arrays","title":"Serving"},{"location":"cli/#assets-management","text":"To list all assets: modelkit assets list To create a new asset: modelkit assets new /path/to/asset asset_category/asset_name To update an asset's minor version: modelkit assets update /path/to/asset asset_category/asset_name To push a new major version: modelkit assets update /path/to/asset asset_category/asset_name --bump-major To retrieve a single asset modelkit assets fetch asset/spec [ --download ] Use --download to force the re-download of the asset.","title":"Assets management"},{"location":"cli/#tf-serving","text":"To configure models from a package to be run in TF serving: modelkit tf-serving local-docker --models [ PACKAGE ] This will write a configuration file with relative paths to the model files. This is meant to be used by mounting the MODELKIT_ASSETS_DIR in the container under the path /config . Other options include: - local-process To create a config file with absolute paths to the assets under MODELKIT_ASSETS_DIR - remote which will use whichever remote paths are found for the assets (i.e. as configured by the MODELKIT_STORAGE_PROVIDER )","title":"TF serving"},{"location":"configuration/","text":"Configuration \u00b6 Environment \u00b6 In order to run/deploy modelkit endpoints, you need to provide it with the necessary environment variables, most of them required by modelkit.assets to retrieve assets from the remote object store: General modelkit environment variables \u00b6 The assets directory is required to know where to find assets MODELKIT_ASSETS_DIR : the local directory in which assets will be downloaded and cached. This needs to be a valid local directory. It is convenient to set a default value of a package in which ModelLibrary will look for models: MODELKIT_DEFAULT_PACKAGE (default None ). It has to be findable (on the PYTHONPATH ) Lazy loading is useful when you want the models to be loaded only when they are actually used. MODELKIT_LAZY_LOADING (defaults to False ) toggles lazy loading mode for the ModelLibrary Storage related environment variables \u00b6 These variables are necessary to set a remote storage from which to retrieve assets. Refer to the storage provider documentation for more information for more information. MODELKIT_STORAGE_BUCKET (default: unset): override storage container where assets are retrieved from. MODELKIT_STORAGE_PREFIX : the prefix under which objects are stored MODELKIT_STORAGE_PROVIDER (default: gcs ) the storage provider (does not have to be set) for MODELKIT_STORAGE_PROVIDER=gcs , the variable GOOGLE_APPLICATION_CREDENTIALS need to be pointing to a service account credentials JSON file (this is not necessary on dev machines) for MODELKIT_STORAGE_PROVIDER=s3 , you need to instantiate AWS_PROFILE for MODELKIT_STORAGE_PROVIDER=az , you need to instantiate AZURE_STORAGE_CONNECTION_STRING with a connection string Assets versioning related environment variable \u00b6 MODELKIT_ASSETS_VERSIONING_SYSTEM will fix the assets versioning system. It can be major_minor or simple_date TF serving environment variables \u00b6 These environment variables can be used to parametrize tensorflow serving. MODELKIT_TF_SERVING_ENABLE (default: True ): Get tensorflow data from tensorflow server, instead of loading these data locally (if set to False you need to install tensorflow). MODELKIT_TF_SERVING_HOST (default: localhost ): IP address of tensorflow server MODELKIT_TF_SERVING_PORT (default: 8501 ): Port of tensorflow server MODELKIT_TF_SERVING_MODE (default: rest ): rest to use REST protocol of tensorflow server (port 8501), grpc to use GRPC protocol (port 8500) TF_SERVING_TIMEOUT_S (default: 60 ): Timeout duration for tensorflow server calls Cache environment variables \u00b6 These environment variables can be used to parametrize the caching. MODELKIT_CACHE_PROVIDER (default: None ) to use prediction caching if MODELKIT_CACHE_PROVIDER=redis , use an external redis instance for caching: MODELKIT_CACHE_HOST (default: localhost ) MODELKIT_CACHE_PORT (default: 6379 ) if MODELKIT_CACHE_PROVIDER=native use native caching (via cachetools ): MODELKIT_CACHE_IMPLEMENTATION can be MODELKIT_CACHE_MAX_SIZE size of the cache","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#environment","text":"In order to run/deploy modelkit endpoints, you need to provide it with the necessary environment variables, most of them required by modelkit.assets to retrieve assets from the remote object store:","title":"Environment"},{"location":"configuration/#general-modelkit-environment-variables","text":"The assets directory is required to know where to find assets MODELKIT_ASSETS_DIR : the local directory in which assets will be downloaded and cached. This needs to be a valid local directory. It is convenient to set a default value of a package in which ModelLibrary will look for models: MODELKIT_DEFAULT_PACKAGE (default None ). It has to be findable (on the PYTHONPATH ) Lazy loading is useful when you want the models to be loaded only when they are actually used. MODELKIT_LAZY_LOADING (defaults to False ) toggles lazy loading mode for the ModelLibrary","title":"General modelkit environment variables"},{"location":"configuration/#storage-related-environment-variables","text":"These variables are necessary to set a remote storage from which to retrieve assets. Refer to the storage provider documentation for more information for more information. MODELKIT_STORAGE_BUCKET (default: unset): override storage container where assets are retrieved from. MODELKIT_STORAGE_PREFIX : the prefix under which objects are stored MODELKIT_STORAGE_PROVIDER (default: gcs ) the storage provider (does not have to be set) for MODELKIT_STORAGE_PROVIDER=gcs , the variable GOOGLE_APPLICATION_CREDENTIALS need to be pointing to a service account credentials JSON file (this is not necessary on dev machines) for MODELKIT_STORAGE_PROVIDER=s3 , you need to instantiate AWS_PROFILE for MODELKIT_STORAGE_PROVIDER=az , you need to instantiate AZURE_STORAGE_CONNECTION_STRING with a connection string","title":"Storage related environment variables"},{"location":"configuration/#assets-versioning-related-environment-variable","text":"MODELKIT_ASSETS_VERSIONING_SYSTEM will fix the assets versioning system. It can be major_minor or simple_date","title":"Assets versioning related environment variable"},{"location":"configuration/#tf-serving-environment-variables","text":"These environment variables can be used to parametrize tensorflow serving. MODELKIT_TF_SERVING_ENABLE (default: True ): Get tensorflow data from tensorflow server, instead of loading these data locally (if set to False you need to install tensorflow). MODELKIT_TF_SERVING_HOST (default: localhost ): IP address of tensorflow server MODELKIT_TF_SERVING_PORT (default: 8501 ): Port of tensorflow server MODELKIT_TF_SERVING_MODE (default: rest ): rest to use REST protocol of tensorflow server (port 8501), grpc to use GRPC protocol (port 8500) TF_SERVING_TIMEOUT_S (default: 60 ): Timeout duration for tensorflow server calls","title":"TF serving environment variables"},{"location":"configuration/#cache-environment-variables","text":"These environment variables can be used to parametrize the caching. MODELKIT_CACHE_PROVIDER (default: None ) to use prediction caching if MODELKIT_CACHE_PROVIDER=redis , use an external redis instance for caching: MODELKIT_CACHE_HOST (default: localhost ) MODELKIT_CACHE_PORT (default: 6379 ) if MODELKIT_CACHE_PROVIDER=native use native caching (via cachetools ): MODELKIT_CACHE_IMPLEMENTATION can be MODELKIT_CACHE_MAX_SIZE size of the cache","title":"Cache environment variables"},{"location":"assets/assets_dir/","text":"When used with a remote storage provider, modelkit will persist assets locally in the assets directory. This is very useful for development, whenever an asset is requested modelkit will first check if it is present before downloading the remote asset if necessary. Because assets are considered immutable , no checks are performed to verify that the objects have not been manually changed locally. Assets directory \u00b6 The local asset directory is found at ASSETS_DIR , although this can also be overrident when instantiating an AssetsManager . Each asset's name is splitted along the path separators as directories, and version information is added. For example, we have pushed to the remote store two assets: a directory to some/directory/asset and a file to some/asset . After retrieving them to the assets_dir , it will look like this: ASSETS_DIR \u2514\u2500\u2500 some | \u251c\u2500\u2500 asset | \u2502 \u251c\u2500\u2500 0.0 # <- the file content | \u2502 \u251c\u2500\u2500 .0.0.SUCCESS # hidden file indicating download success | \u2502 \u251c\u2500\u2500 0.1 | \u2502 | ... | \u251c\u2500\u2500 directory | \u2502 \u251c\u2500\u2500 asset | \u2502 \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u2502 | \u251c\u2500\u2500 .SUCCESS # hidden file indicating download success | \u2502 \u2502 | \u251c\u2500\u2500 content0 <- the directory contents | \u2502 \u2502 | \u251c\u2500\u2500 content2 | \u2502 \u2502 | | ... | \u2502 \u2502 \u251c\u2500\u2500 0.1 | \u2502 \u2502 | | ... | ... Note All previous versions of the assets are kept locally. It is safe, however to delete local copies of the assets manually to save space. For directory assets, delete the version directory. For file assets, do not forget to delete the .version.SUCCESS file too. To retrieve the assets path, refer to it via its asset specification: mng = AssetsManager ( assets_dir = assets_dir ) mng . fetch_asset ( \"some/asset:0.0\" ) # will point to `ASSETS_DIR/some/asset/0.0` mng . fetch_asset ( \"some/directory/asset:0.1\" ) # will point to `ASSETS_DIR/some/asset/0.1` mng . fetch_asset ( \"some/directory/asset:0.1[content0]\" ) # will point to `ASSETS_DIR/some/asset/0.1/content0` Version resolution \u00b6 When an asset is requested with the version not fully specified, modelkit may need to consult the remote storage to find the latest version. As a result, modelkit 's asset manager will, in this context, have a different behavior depending on whether a remote storage provider is parametrized: Without a remote store find the latest version of the asset available in the ASSETS_DIR With a remote store contact the remote store to find the latest version, check whether it is present locally. If it is, use the local version, otherwise download the latest version. This has an important consequence, which is that unpinned assets will always require network calls when being fetched, although they may already be present. For all production purposes, you should pin your asset versions .","title":"Local assets dir"},{"location":"assets/assets_dir/#assets-directory","text":"The local asset directory is found at ASSETS_DIR , although this can also be overrident when instantiating an AssetsManager . Each asset's name is splitted along the path separators as directories, and version information is added. For example, we have pushed to the remote store two assets: a directory to some/directory/asset and a file to some/asset . After retrieving them to the assets_dir , it will look like this: ASSETS_DIR \u2514\u2500\u2500 some | \u251c\u2500\u2500 asset | \u2502 \u251c\u2500\u2500 0.0 # <- the file content | \u2502 \u251c\u2500\u2500 .0.0.SUCCESS # hidden file indicating download success | \u2502 \u251c\u2500\u2500 0.1 | \u2502 | ... | \u251c\u2500\u2500 directory | \u2502 \u251c\u2500\u2500 asset | \u2502 \u2502 \u251c\u2500\u2500 0.0 | \u2502 \u2502 | \u251c\u2500\u2500 .SUCCESS # hidden file indicating download success | \u2502 \u2502 | \u251c\u2500\u2500 content0 <- the directory contents | \u2502 \u2502 | \u251c\u2500\u2500 content2 | \u2502 \u2502 | | ... | \u2502 \u2502 \u251c\u2500\u2500 0.1 | \u2502 \u2502 | | ... | ... Note All previous versions of the assets are kept locally. It is safe, however to delete local copies of the assets manually to save space. For directory assets, delete the version directory. For file assets, do not forget to delete the .version.SUCCESS file too. To retrieve the assets path, refer to it via its asset specification: mng = AssetsManager ( assets_dir = assets_dir ) mng . fetch_asset ( \"some/asset:0.0\" ) # will point to `ASSETS_DIR/some/asset/0.0` mng . fetch_asset ( \"some/directory/asset:0.1\" ) # will point to `ASSETS_DIR/some/asset/0.1` mng . fetch_asset ( \"some/directory/asset:0.1[content0]\" ) # will point to `ASSETS_DIR/some/asset/0.1/content0`","title":"Assets directory"},{"location":"assets/assets_dir/#version-resolution","text":"When an asset is requested with the version not fully specified, modelkit may need to consult the remote storage to find the latest version. As a result, modelkit 's asset manager will, in this context, have a different behavior depending on whether a remote storage provider is parametrized: Without a remote store find the latest version of the asset available in the ASSETS_DIR With a remote store contact the remote store to find the latest version, check whether it is present locally. If it is, use the local version, otherwise download the latest version. This has an important consequence, which is that unpinned assets will always require network calls when being fetched, although they may already be present. For all production purposes, you should pin your asset versions .","title":"Version resolution"},{"location":"assets/environment/","text":"Environment \u00b6 Model library \u00b6 The only necessary environment variable to set to use modelkit is the assets directory MODELKIT_ASSETS_DIR it has to be a valid local directory. AssetsManager settings \u00b6 The parameters necessary to instantiate an AssetsManager can all be read from environment variables, or provided when initializing the AssetsManager . Environment variable Default value Parameter Notes MODELKIT_STORAGE_PROVIDER gcs storage_provider gcs (default), s3 , az or local MODELKIT_STORAGE_BUCKET None bucket Bucket in which data is stored MODELKIT_STORAGE_PREFIX modelkit-assets prefix Objects prefix MODELKIT_STORAGE_TIMEOUT_S 300 timeout_s max time when retrying storage downloads MODELKIT_ASSETS_TIMEOUT_S 10 timeout file lock timeout when downloading assets More settings can be passed in order to configure the driver itself, see the storage provider documentation for more information","title":"Environment"},{"location":"assets/environment/#environment","text":"","title":"Environment"},{"location":"assets/environment/#model-library","text":"The only necessary environment variable to set to use modelkit is the assets directory MODELKIT_ASSETS_DIR it has to be a valid local directory.","title":"Model library"},{"location":"assets/environment/#assetsmanager-settings","text":"The parameters necessary to instantiate an AssetsManager can all be read from environment variables, or provided when initializing the AssetsManager . Environment variable Default value Parameter Notes MODELKIT_STORAGE_PROVIDER gcs storage_provider gcs (default), s3 , az or local MODELKIT_STORAGE_BUCKET None bucket Bucket in which data is stored MODELKIT_STORAGE_PREFIX modelkit-assets prefix Objects prefix MODELKIT_STORAGE_TIMEOUT_S 300 timeout_s max time when retrying storage downloads MODELKIT_ASSETS_TIMEOUT_S 10 timeout file lock timeout when downloading assets More settings can be passed in order to configure the driver itself, see the storage provider documentation for more information","title":"AssetsManager settings"},{"location":"assets/managing_assets/","text":"This section describes how to push assets either manually using CLI or programmatically. Asset maintenance actions \u00b6 Since assets are immutable, there are only two actions one can take to affect the remotely stored assets. update an existing asset: If the asset name already exists remotely, the appropriate action is to update it. create a new asset: If this is the first time that this asset asset name is created, the correct action is the create it. modelkit does not offer ways to delete or replace assets. Maintaining assets with CLI \u00b6 modelkit implements CLIs to ease the maintenance of remote assets. Make sure the storage provider is properly setup with environment variables (see here ). Create a new asset \u00b6 To create a new asset: modelkit assets new /path/to/asset/locally asset_name After prompting you for confirmation, it will create a remote asset with initial version. (e.g. 0.0 for default major/minor versioning system, or the current date for \"simple date\" system) Update an asset \u00b6 Use modelkit assets update to update an existing asset using a local file or directory at /local/asset/path under a new version according to the version system. Major/minor versioning system \u00b6 Bump the minor version Assuming name has versions 0.1 , 1.1 , running modelkit assets update /local/asset/path name will add a version 1.2 Bump the major version Assuming name has versions 0.1 , 1.0 , running modelkit assets update /local/asset/path name --bump-major After prompting your for confirmation, it will add a version 2.0 Bump the minor version of an older asset Assuming name has versions 0.1 , 1.0 , running modelkit assets update /local/asset/path name:0 will add a version 0.2 Simple date system \u00b6 modelkit assets update /local/asset/path name will bump a new version equivalent to the current UTC date in iso format YYYY-MM-DDThh-mm-ssZ Listing remote assets \u00b6 A CLI is also available to list remote assets in a given bucket: modelkit assets list Maintaining assets programmatically \u00b6 First, instantiate an AssetsManager pointing to the desired bucket , possibly changing the prefix , storage method, etc.: from modelkit.assets.remote import StorageProvider assets_store = StorageProvider () Create a new asset \u00b6 Assuming the asset is locally present at asset_path (either a file or a directory), create the remote asset name:initial_version as follows: assets_store . new ( asset_path , initial_version , name ) where initial_version is obtained using the get_initial_version method of your AssetsVersioningSystem system Important Creating new assets programmatically is possible, even though it is not considered a good practice. Using the CLI is the prefered and safest way to manage assets. Update the asset \u00b6 Assuming the asset is locally present at asset_path (either a file or a directory), update the remote asset name as follows: assets_store . update ( asset_path , name , version , ) where version is the new version which can be updated using the increment_version method of your AssetsVersioningSystem system","title":"Managing assets"},{"location":"assets/managing_assets/#asset-maintenance-actions","text":"Since assets are immutable, there are only two actions one can take to affect the remotely stored assets. update an existing asset: If the asset name already exists remotely, the appropriate action is to update it. create a new asset: If this is the first time that this asset asset name is created, the correct action is the create it. modelkit does not offer ways to delete or replace assets.","title":"Asset maintenance actions"},{"location":"assets/managing_assets/#maintaining-assets-with-cli","text":"modelkit implements CLIs to ease the maintenance of remote assets. Make sure the storage provider is properly setup with environment variables (see here ).","title":"Maintaining assets with CLI"},{"location":"assets/managing_assets/#create-a-new-asset","text":"To create a new asset: modelkit assets new /path/to/asset/locally asset_name After prompting you for confirmation, it will create a remote asset with initial version. (e.g. 0.0 for default major/minor versioning system, or the current date for \"simple date\" system)","title":"Create a new asset"},{"location":"assets/managing_assets/#update-an-asset","text":"Use modelkit assets update to update an existing asset using a local file or directory at /local/asset/path under a new version according to the version system.","title":"Update an asset"},{"location":"assets/managing_assets/#majorminor-versioning-system","text":"Bump the minor version Assuming name has versions 0.1 , 1.1 , running modelkit assets update /local/asset/path name will add a version 1.2 Bump the major version Assuming name has versions 0.1 , 1.0 , running modelkit assets update /local/asset/path name --bump-major After prompting your for confirmation, it will add a version 2.0 Bump the minor version of an older asset Assuming name has versions 0.1 , 1.0 , running modelkit assets update /local/asset/path name:0 will add a version 0.2","title":"Major/minor versioning system"},{"location":"assets/managing_assets/#simple-date-system","text":"modelkit assets update /local/asset/path name will bump a new version equivalent to the current UTC date in iso format YYYY-MM-DDThh-mm-ssZ","title":"Simple date system"},{"location":"assets/managing_assets/#listing-remote-assets","text":"A CLI is also available to list remote assets in a given bucket: modelkit assets list","title":"Listing remote assets"},{"location":"assets/managing_assets/#maintaining-assets-programmatically","text":"First, instantiate an AssetsManager pointing to the desired bucket , possibly changing the prefix , storage method, etc.: from modelkit.assets.remote import StorageProvider assets_store = StorageProvider ()","title":"Maintaining assets programmatically"},{"location":"assets/managing_assets/#create-a-new-asset_1","text":"Assuming the asset is locally present at asset_path (either a file or a directory), create the remote asset name:initial_version as follows: assets_store . new ( asset_path , initial_version , name ) where initial_version is obtained using the get_initial_version method of your AssetsVersioningSystem system Important Creating new assets programmatically is possible, even though it is not considered a good practice. Using the CLI is the prefered and safest way to manage assets.","title":"Create a new asset"},{"location":"assets/managing_assets/#update-the-asset","text":"Assuming the asset is locally present at asset_path (either a file or a directory), update the remote asset name as follows: assets_store . update ( asset_path , name , version , ) where version is the new version which can be updated using the increment_version method of your AssetsVersioningSystem system","title":"Update the asset"},{"location":"assets/remote_assets/","text":"Remote assets allow you to share the necessary files and folders necessary to run your models with other members of your team, as well as with production services. In addition modelkit helps with the versioning of these files, and the management of your local developer copies. Remote asset properties \u00b6 For modelkit remote assets are immutable , and their source of truth has to be a single remote object store. These have the following advantages: auditing it is possible to know which asset was used when by a production service, and push by whom reverting it is always possible to revert to an older version of a Model because assets will be present and cannot be modified. loss of data local machines can lose all of their data, assets will always be available from the remote store reproducibility the code running on your local machine is guaranteed to use the same artifacts and code as the one running in production Although these come at a cost, modelkit helps you manage, update, and create new assets. It also helps with maintaining your local copies of the assets to make development quicker (in the assets directory ). Model with remote asset \u00b6 Using a remote asset in a Model is exactly the same thing as using a local one and using _load as we have seen before . We add a valid remote asset specification as a key in the configuration, modelkit will make sure to retrieve it before the Model is instantiated: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"test/asset:1\" } # meaning \"version 1 of test/asset\" } Assuming that you have parametrized a GCS object store, this will cause the ModelLibrary to: download the objects at gs://some-bucket-name/assets/test/1/yolo/* locally (which storage provider and bucket is used depends on your configuration ) write the files to the assets directory (controlled by ASSETS_DIR ) set the Model.asset_path attribute accordingly. As a result, you can still write your own arbitrary _load logic, with confidence that the asset will actually be here class ModelWithAsset ( Model ): def _load ( self ): # For example, here, the asset is a BZ2 compressed JSON file with bz2 . BZ2File ( self . asset_path , \"rb\" ) as f : self . data_structure = pickle . load ( f ) # loads {\"response\": \"Hello World!\"} def _predict ( self , item , ** kwargs ): return self . data_structure [ \"response\" ] # returns \"Hello World!\" Asset specification \u00b6 An asset specification string follows the convention: name/of/asset/object:version[/asset/subobject] Where: the name of the asset has to be a valid object store name (using / as a prefix separator) version follows a semantic versioning system: (by default major.minor (e.g. 1.2 )) [/asset/subobject] optionally allows one to refer directly to a `sub object Version resolution \u00b6 Whenever a version is not completely set, the missing information is resolved to the latest version. For example: name/of/asset/object is resolved to the latest version altogether Some versioning system can support partial version setting Example for major/minor system : name/of/asset/object:1 is resolved to the latest minor version 1.*","title":"Remote assets"},{"location":"assets/remote_assets/#remote-asset-properties","text":"For modelkit remote assets are immutable , and their source of truth has to be a single remote object store. These have the following advantages: auditing it is possible to know which asset was used when by a production service, and push by whom reverting it is always possible to revert to an older version of a Model because assets will be present and cannot be modified. loss of data local machines can lose all of their data, assets will always be available from the remote store reproducibility the code running on your local machine is guaranteed to use the same artifacts and code as the one running in production Although these come at a cost, modelkit helps you manage, update, and create new assets. It also helps with maintaining your local copies of the assets to make development quicker (in the assets directory ).","title":"Remote asset properties"},{"location":"assets/remote_assets/#model-with-remote-asset","text":"Using a remote asset in a Model is exactly the same thing as using a local one and using _load as we have seen before . We add a valid remote asset specification as a key in the configuration, modelkit will make sure to retrieve it before the Model is instantiated: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"test/asset:1\" } # meaning \"version 1 of test/asset\" } Assuming that you have parametrized a GCS object store, this will cause the ModelLibrary to: download the objects at gs://some-bucket-name/assets/test/1/yolo/* locally (which storage provider and bucket is used depends on your configuration ) write the files to the assets directory (controlled by ASSETS_DIR ) set the Model.asset_path attribute accordingly. As a result, you can still write your own arbitrary _load logic, with confidence that the asset will actually be here class ModelWithAsset ( Model ): def _load ( self ): # For example, here, the asset is a BZ2 compressed JSON file with bz2 . BZ2File ( self . asset_path , \"rb\" ) as f : self . data_structure = pickle . load ( f ) # loads {\"response\": \"Hello World!\"} def _predict ( self , item , ** kwargs ): return self . data_structure [ \"response\" ] # returns \"Hello World!\"","title":"Model with remote asset"},{"location":"assets/remote_assets/#asset-specification","text":"An asset specification string follows the convention: name/of/asset/object:version[/asset/subobject] Where: the name of the asset has to be a valid object store name (using / as a prefix separator) version follows a semantic versioning system: (by default major.minor (e.g. 1.2 )) [/asset/subobject] optionally allows one to refer directly to a `sub object","title":"Asset specification"},{"location":"assets/remote_assets/#version-resolution","text":"Whenever a version is not completely set, the missing information is resolved to the latest version. For example: name/of/asset/object is resolved to the latest version altogether Some versioning system can support partial version setting Example for major/minor system : name/of/asset/object:1 is resolved to the latest minor version 1.*","title":"Version resolution"},{"location":"assets/retrieving_assets/","text":"It is possible to access assets programmatically from their asset specification Once you have correctly configured your environment and storage provider : from modelkit.assets.manager import AssetsManager mng = AssetsManager () asset_path = mng . fetch_asset ( \"asset_category/asset_name:version[sub/part]\" ) with open ( asset_path , \"r\" ) as f : # do something with the asset ... By default, AssetsManager.fetch_asset only returns the path to the locally downloaded asset, but it can return more information about the fetched asset if provided with the return_info=True . In this case it returns a dictionary with: { \"path\" : \"/local/path/to/asset\" , \"from_cache\" : True or False , # whether the asset was pulled from cache, \"version\" : \"returned asset version\" , # the asset version # These are present only when the asset was # downloaded from the remote store: \"meta\" : {}, # contents of the meta JSON object # remote object names \"object_name\" : \"remote object name\" , \"meta_object_name\" : \"remote meta object name\" , \"versions_object_name\" : \"remote version object name\" }","title":"Retrieving assets"},{"location":"assets/storage_provider/","text":"In order to take advantage of remote asset storage, you have to configure your environment to use the right storage provider . This is generally done by means of environment variables, and currently supports object stores on S3 (e.g. AWS, or minio) or GCS. The first thing you will need is a local directory in which assets will be retrieved and stored. This is best set in an environment variable MODELKIT_ASSETS_DIR which has to point to an existing directory. Remote storage paths \u00b6 You will need a remote object store, as identified by a bucket, and modelkit will store all objects under a given prefix. These are controlled by the following environment variables MODELKIT_STORAGE_BUCKET the name of the buket MODELKIT_STORAGE_PREFIX the prefix of all modelkit objects in the bucket Permissions \u00b6 You will need to have credentials present with permissions. At runtime \u00b6 This is typically the case of running services, they need read access to all the objects in the bucket under the storage prefix. For developers \u00b6 Developers may additionally need to be able to push new assets and or update existing assets, which requires them to be able to create and update certain objects. Using different providers \u00b6 The flavor of the remote store that is used depends on the MODELKIT_STORAGE_PROVIDER environment variables Using AWS S3 storage \u00b6 Use MODELKIT_STORAGE_PROVIDER=s3 to connect to S3 storage. We use boto3 under the hood. The authentication information here is passed to the boto3.client object: Environment variable boto3.client argument AWS_ACCESS_KEY_ID aws_access_key_id AWS_SECRET_ACCESS_KEY aws_secret_access_key AWS_SESSION_TOKEN aws_session_token AWS_DEFAULT_REGION region_name S3_ENDPOINT endpoint_url Typically, if you use AWS: having AWS_DEFAULT_PROFILE , AWS_DEFAULT_REGION and valid credentials in ~/.aws is enough. S3 storage driver is compatible with KMS encrypted s3 volumes. Use AWS_KMS_KEY_ID environment variable to set your key and be able to upload files to such volume. GCS storage \u00b6 Use MODELKIT_STORAGE_PROVIDER=gcs to connect to GCS storage. We use google-cloud-storage . Environment variable Default value Notes GOOGLE_APPLICATION_CREDENTIALS None path to the JSON file By default, the GCS client use the credentials setup up on the machine. If GOOGLE_APPLICATION_CREDENTIALS is provided, it should point to a local JSON service account file, which we use to instantiate the client with google.cloud.storage.Client.from_service_account_json Using Azure blob storage \u00b6 Use MODELKIT_STORAGE_PROVIDER=az to connect to Azure blob storage. We use azure-storage-blobl under the hood. The client is created by passing the authentication information to BlobServiceClient.from_connection_string : Environment variable Note AZURE_STORAGE_CONNECTION_STRING azure connection string local mode \u00b6 Use MODELKIT_STORAGE_PROVIDER=local to treat a local folder as a remote source. Assets will be downloaded from this folder to the configured asset dir. If you would like to run on already downloaded assets please refer to \"Pre-downloaded mode\" section below. Environment variable Notes MODELKIT_STORAGE_BUCKET path to the local folder MODELKIT_STORAGE_PREFIX sub directory where assets are fetched from Pre-downloaded mode \u00b6 Use MODELKIT_STORAGE_PROVIDER= or unset it to use only assets available in assets dir. This can be used for local development or for deploying assets in read-only artifacts. Other options \u00b6 If you would like us to support other means of remote storage, do feel free to request it by posting an issue !","title":"Storage provider"},{"location":"assets/storage_provider/#remote-storage-paths","text":"You will need a remote object store, as identified by a bucket, and modelkit will store all objects under a given prefix. These are controlled by the following environment variables MODELKIT_STORAGE_BUCKET the name of the buket MODELKIT_STORAGE_PREFIX the prefix of all modelkit objects in the bucket","title":"Remote storage paths"},{"location":"assets/storage_provider/#permissions","text":"You will need to have credentials present with permissions.","title":"Permissions"},{"location":"assets/storage_provider/#at-runtime","text":"This is typically the case of running services, they need read access to all the objects in the bucket under the storage prefix.","title":"At runtime"},{"location":"assets/storage_provider/#for-developers","text":"Developers may additionally need to be able to push new assets and or update existing assets, which requires them to be able to create and update certain objects.","title":"For developers"},{"location":"assets/storage_provider/#using-different-providers","text":"The flavor of the remote store that is used depends on the MODELKIT_STORAGE_PROVIDER environment variables","title":"Using different providers"},{"location":"assets/storage_provider/#using-aws-s3-storage","text":"Use MODELKIT_STORAGE_PROVIDER=s3 to connect to S3 storage. We use boto3 under the hood. The authentication information here is passed to the boto3.client object: Environment variable boto3.client argument AWS_ACCESS_KEY_ID aws_access_key_id AWS_SECRET_ACCESS_KEY aws_secret_access_key AWS_SESSION_TOKEN aws_session_token AWS_DEFAULT_REGION region_name S3_ENDPOINT endpoint_url Typically, if you use AWS: having AWS_DEFAULT_PROFILE , AWS_DEFAULT_REGION and valid credentials in ~/.aws is enough. S3 storage driver is compatible with KMS encrypted s3 volumes. Use AWS_KMS_KEY_ID environment variable to set your key and be able to upload files to such volume.","title":"Using AWS S3 storage"},{"location":"assets/storage_provider/#gcs-storage","text":"Use MODELKIT_STORAGE_PROVIDER=gcs to connect to GCS storage. We use google-cloud-storage . Environment variable Default value Notes GOOGLE_APPLICATION_CREDENTIALS None path to the JSON file By default, the GCS client use the credentials setup up on the machine. If GOOGLE_APPLICATION_CREDENTIALS is provided, it should point to a local JSON service account file, which we use to instantiate the client with google.cloud.storage.Client.from_service_account_json","title":"GCS storage"},{"location":"assets/storage_provider/#using-azure-blob-storage","text":"Use MODELKIT_STORAGE_PROVIDER=az to connect to Azure blob storage. We use azure-storage-blobl under the hood. The client is created by passing the authentication information to BlobServiceClient.from_connection_string : Environment variable Note AZURE_STORAGE_CONNECTION_STRING azure connection string","title":"Using Azure blob storage"},{"location":"assets/storage_provider/#local-mode","text":"Use MODELKIT_STORAGE_PROVIDER=local to treat a local folder as a remote source. Assets will be downloaded from this folder to the configured asset dir. If you would like to run on already downloaded assets please refer to \"Pre-downloaded mode\" section below. Environment variable Notes MODELKIT_STORAGE_BUCKET path to the local folder MODELKIT_STORAGE_PREFIX sub directory where assets are fetched from","title":"local mode"},{"location":"assets/storage_provider/#pre-downloaded-mode","text":"Use MODELKIT_STORAGE_PROVIDER= or unset it to use only assets available in assets dir. This can be used for local development or for deploying assets in read-only artifacts.","title":"Pre-downloaded mode"},{"location":"assets/storage_provider/#other-options","text":"If you would like us to support other means of remote storage, do feel free to request it by posting an issue !","title":"Other options"},{"location":"assets/store_organization/","text":"Remote asset storage convention \u00b6 Data object \u00b6 Remote assets are stored in object stores, referenced as: [provider]://[bucket]/[prefix]/[category]/[name]/[version] In this \"path\": provider is s3 , azfs or gcs or file depending on the storage driver (value of MODELKIT_STORAGE_PROVIDER ) bucket is the remote container name ( MODELKIT_STORAGE_BUCKET ) The rest of the \"path\" is the remote object's name and consists of -prefix is a prefix to all assets for a given AssetsManager ( MODELKIT_STORAGE_PREFIX ) name describes the asset. The name may contain path separators / but each file remotely will be stored as a single object. version describes the asset version in the form X.Y Note If the asset is a directory, all sub files will be stored as separate objects under this prefix. Meta object \u00b6 In addition to the data, the asset object reside alongside a *.meta object: [provider]://[bucket]/[prefix]/[category]/[name]/[version].meta The meta is a JSON file containing { \"push_date\" : \"\" , # ISO date of push \"is_directory\" : True or False , # whether the asset has mulitple objects \"contents\" : [] # list of suffixes of contents when is_directory is True } Version object \u00b6 Assets have versions, following a Major.Minor version convention. We maintain a versions JSON file in order to keep track of the latest version. This avoids having to list objects by prefix on objects store, which is typically a very time consuming query. It is stored at [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name].versions And contains { \"versions\": [], # list of ordered versions, latest first } Note This is the only object on the object store that is not immutable . It is updated whenever an asset is updated.","title":"Remote store organization"},{"location":"assets/store_organization/#remote-asset-storage-convention","text":"","title":"Remote asset storage convention"},{"location":"assets/store_organization/#data-object","text":"Remote assets are stored in object stores, referenced as: [provider]://[bucket]/[prefix]/[category]/[name]/[version] In this \"path\": provider is s3 , azfs or gcs or file depending on the storage driver (value of MODELKIT_STORAGE_PROVIDER ) bucket is the remote container name ( MODELKIT_STORAGE_BUCKET ) The rest of the \"path\" is the remote object's name and consists of -prefix is a prefix to all assets for a given AssetsManager ( MODELKIT_STORAGE_PREFIX ) name describes the asset. The name may contain path separators / but each file remotely will be stored as a single object. version describes the asset version in the form X.Y Note If the asset is a directory, all sub files will be stored as separate objects under this prefix.","title":"Data object"},{"location":"assets/store_organization/#meta-object","text":"In addition to the data, the asset object reside alongside a *.meta object: [provider]://[bucket]/[prefix]/[category]/[name]/[version].meta The meta is a JSON file containing { \"push_date\" : \"\" , # ISO date of push \"is_directory\" : True or False , # whether the asset has mulitple objects \"contents\" : [] # list of suffixes of contents when is_directory is True }","title":"Meta object"},{"location":"assets/store_organization/#version-object","text":"Assets have versions, following a Major.Minor version convention. We maintain a versions JSON file in order to keep track of the latest version. This avoids having to list objects by prefix on objects store, which is typically a very time consuming query. It is stored at [provider]://[bucket]/[assetsmanager-prefix]/[category]/[name].versions And contains { \"versions\": [], # list of ordered versions, latest first } Note This is the only object on the object store that is not immutable . It is updated whenever an asset is updated.","title":"Version object"},{"location":"assets/versioning/","text":"Versioning \u00b6 2 versioning systems are implemented by default in the modelkit/assets/versioning repository : major_minor (used by default) simple_date We can implement a new versioning system by inheriting from the AssetsVersioningSystem class and override several methods which are necessary for the system to function. To use a specific system use the environment variable MODELKIT_ASSETS_VERSIONING_SYSTEM AssetsVersioningSystem \u00b6 Some abstract functions have to be overriden : get_initial_version which returns the initial version of a new asset. check_version_valid which checks if a given version is valid. sort_versions which implements the sorting logic of versions (used for example to get the latest version). increment_version which implements the version incrementation logic. get_update_cli_params which specifies the update cli display and the increment_version params received from parameters given to the update cli . other methods can be overriden if needed. is_version_complete returns True by default but can be overriden for system which allows incomplete version to specify if a given is complete or not. get_latest_partial_version returns the last version by default but can be overriden for system which allow incomplete version to filter versions corresponding to a given incomplete version. Simple Date \u00b6 Simple date is a very simple versioning system using current date in UTC ISO 8601 Z format as version : YYYY-MM-DDThh-mm-ssZ (due to filename constraints the : of hh:mm:ss in the original notation are replaced with - ). get_initial_version returns the current date in UTC iso format. increment_version returns the current date in UTC iso format. sort_versions iso format is compatible with a reversed alpha-numerical order. Major / Minor (default) \u00b6 The major / minor system uses a x.y format where x is the major and y the minor version. get_initial_version returns 0.0 . increment_version increments the minor version ( y+=1 ). A bump_version parameter allow us to increment a new major version ( x+=1 ; y=0 ). sort_versions sorts according to a (major, minor) key, considering major and minor as integers. major/minor is compatible with incomplete versioning : version x corresponds to the latest x.y version : is_version_complete returns True if major AND minor are specified and False if only the major is specified. get_latest_partial_version returns the latest version OR the latest x.y version for a specified major x .","title":"Versioning systems"},{"location":"assets/versioning/#versioning","text":"2 versioning systems are implemented by default in the modelkit/assets/versioning repository : major_minor (used by default) simple_date We can implement a new versioning system by inheriting from the AssetsVersioningSystem class and override several methods which are necessary for the system to function. To use a specific system use the environment variable MODELKIT_ASSETS_VERSIONING_SYSTEM","title":"Versioning"},{"location":"assets/versioning/#assetsversioningsystem","text":"Some abstract functions have to be overriden : get_initial_version which returns the initial version of a new asset. check_version_valid which checks if a given version is valid. sort_versions which implements the sorting logic of versions (used for example to get the latest version). increment_version which implements the version incrementation logic. get_update_cli_params which specifies the update cli display and the increment_version params received from parameters given to the update cli . other methods can be overriden if needed. is_version_complete returns True by default but can be overriden for system which allows incomplete version to specify if a given is complete or not. get_latest_partial_version returns the last version by default but can be overriden for system which allow incomplete version to filter versions corresponding to a given incomplete version.","title":"AssetsVersioningSystem"},{"location":"assets/versioning/#simple-date","text":"Simple date is a very simple versioning system using current date in UTC ISO 8601 Z format as version : YYYY-MM-DDThh-mm-ssZ (due to filename constraints the : of hh:mm:ss in the original notation are replaced with - ). get_initial_version returns the current date in UTC iso format. increment_version returns the current date in UTC iso format. sort_versions iso format is compatible with a reversed alpha-numerical order.","title":"Simple Date"},{"location":"assets/versioning/#major-minor-default","text":"The major / minor system uses a x.y format where x is the major and y the minor version. get_initial_version returns 0.0 . increment_version increments the minor version ( y+=1 ). A bump_version parameter allow us to increment a new major version ( x+=1 ; y=0 ). sort_versions sorts according to a (major, minor) key, considering major and minor as integers. major/minor is compatible with incomplete versioning : version x corresponds to the latest x.y version : is_version_complete returns True if major AND minor are specified and False if only the major is specified. get_latest_partial_version returns the latest version OR the latest x.y version for a specified major x .","title":"Major / Minor (default)"},{"location":"deployment/deployment/","text":"modelkit centralizes all of your models in a single object, which makes it easy to serve as a REST API via an HTTP server. Of course, you can do so using your favourite framework, ours is fastapi , so we have integrated several methods to make it easy to serve your models directly with it. Using uvicorn \u00b6 A single CLI call will expose your models using uvicorn : modelkit serve PACKAGE --required-models REQUIRED_MODELS --host HOST --port PORT This will create a server with a single worker with all the models listed in REQUIRED_MODELS , as configured in the PACKAGE . Each Model configured as model_name will have two POST endpoints: /predict/model_name which accepts single items /predict/batch/model_name which accepts lists of items Endpoint payloads and specs are parametrized by the pydantic payloads that it uses to validate its inputs and outputs. Head over to /docs to check out the swagger and try your models out! Using gunicorn with multiple workers \u00b6 For more performance, modelkit allows you to use gunicorn with multiple workers that share the same ModelLibrary : modelkit.api.create_modelkit_app . It can take all its arguments through environment variables, so you can run a server quickly, for example: export MODELKIT_DEFAULT_PACKAGE = my_package export PYTHONPATH = path/to/the/package gunicorn \\ --workers 4 \\ --preload \\ --worker-class = uvicorn.workers.UvicornWorker \\ 'modelkit.api:create_modelkit_app()' Note Since ModelLibrary is shared between the workers, therefore adding workers will not increase the memory footprint. Automatic endpoints router in fastAPI \u00b6 If you are interested in adding all Model endpoints in an existing fastapi application, you can also use the modelkit.api.ModelkitAutoAPIRouter : app = fastapi . FastAPI () router = ModelkitAutoAPIRouter ( required_models = required_models , models = models ) app . include_router ( router ) Which will include one endpoint for each model in required_models , pulled form the models package. To override the route paths for individual models, use route_paths : router = ModelkitAutoAPIRouter ( required_models = required_models , models = models , route_paths = { \"some_model\" : \"/a/new/path\" } )","title":"Automatic"},{"location":"deployment/deployment/#using-uvicorn","text":"A single CLI call will expose your models using uvicorn : modelkit serve PACKAGE --required-models REQUIRED_MODELS --host HOST --port PORT This will create a server with a single worker with all the models listed in REQUIRED_MODELS , as configured in the PACKAGE . Each Model configured as model_name will have two POST endpoints: /predict/model_name which accepts single items /predict/batch/model_name which accepts lists of items Endpoint payloads and specs are parametrized by the pydantic payloads that it uses to validate its inputs and outputs. Head over to /docs to check out the swagger and try your models out!","title":"Using uvicorn"},{"location":"deployment/deployment/#using-gunicorn-with-multiple-workers","text":"For more performance, modelkit allows you to use gunicorn with multiple workers that share the same ModelLibrary : modelkit.api.create_modelkit_app . It can take all its arguments through environment variables, so you can run a server quickly, for example: export MODELKIT_DEFAULT_PACKAGE = my_package export PYTHONPATH = path/to/the/package gunicorn \\ --workers 4 \\ --preload \\ --worker-class = uvicorn.workers.UvicornWorker \\ 'modelkit.api:create_modelkit_app()' Note Since ModelLibrary is shared between the workers, therefore adding workers will not increase the memory footprint.","title":"Using gunicorn with multiple workers"},{"location":"deployment/deployment/#automatic-endpoints-router-in-fastapi","text":"If you are interested in adding all Model endpoints in an existing fastapi application, you can also use the modelkit.api.ModelkitAutoAPIRouter : app = fastapi . FastAPI () router = ModelkitAutoAPIRouter ( required_models = required_models , models = models ) app . include_router ( router ) Which will include one endpoint for each model in required_models , pulled form the models package. To override the route paths for individual models, use route_paths : router = ModelkitAutoAPIRouter ( required_models = required_models , models = models , route_paths = { \"some_model\" : \"/a/new/path\" } )","title":"Automatic endpoints router in fastAPI"},{"location":"deployment/integrate_fastapi/","text":"Integrating modelkit models in an existing app is extremely simple, you simply have to add the ModelLibrary object to the main module. Instantiate a ModelLibrary \u00b6 In the main Python script where you define your fastapi application, attach the model library to the application state: import fastapi import modelkit app = fastapi . FastAPI () # instantiate the library lib = modelkit . ModelLibrary ( ... ) # add the library to the app state app . state . lib = lib # Don't forget to close the connections when the application stops! @app . on_event ( \"shutdown\" ) async def shutdown_event (): await app . state . lib . aclose () Multiple workers This method of integrating the ModelLibrary ensures that all models are created before different workers are instantiated (e.g. using gunicorn --preload ), which is convenient since all will share the same model objects and not increase memory. Use the models \u00b6 Finally, anywhere else where you add endpoints to the application, you can retrieve the ModelLibrary from the request object. Since getting the Model object is just a dictionary lookup, retrieving it is instantaneous. @app . post ( ... ) def some_path_endpoint ( request : fastapi . Request , item ): # Get the model object m = request . app . state . lib . get ( \"model_name\" ) # Use to make predictions as usual m . predict ( ... ) ... return ... Async support This is the context in which modelkit 's async support shines, be sure to use your AsyncModel s here: @app . post ( ... ) async def some_path_endpoint ( request : fastapi . Request , item ): m = request . app . state . lib . get ( \"model_name\" ) result = await m . predict ( ... ) ... return ...","title":"Integrate in existing app"},{"location":"deployment/integrate_fastapi/#instantiate-a-modellibrary","text":"In the main Python script where you define your fastapi application, attach the model library to the application state: import fastapi import modelkit app = fastapi . FastAPI () # instantiate the library lib = modelkit . ModelLibrary ( ... ) # add the library to the app state app . state . lib = lib # Don't forget to close the connections when the application stops! @app . on_event ( \"shutdown\" ) async def shutdown_event (): await app . state . lib . aclose () Multiple workers This method of integrating the ModelLibrary ensures that all models are created before different workers are instantiated (e.g. using gunicorn --preload ), which is convenient since all will share the same model objects and not increase memory.","title":"Instantiate a ModelLibrary"},{"location":"deployment/integrate_fastapi/#use-the-models","text":"Finally, anywhere else where you add endpoints to the application, you can retrieve the ModelLibrary from the request object. Since getting the Model object is just a dictionary lookup, retrieving it is instantaneous. @app . post ( ... ) def some_path_endpoint ( request : fastapi . Request , item ): # Get the model object m = request . app . state . lib . get ( \"model_name\" ) # Use to make predictions as usual m . predict ( ... ) ... return ... Async support This is the context in which modelkit 's async support shines, be sure to use your AsyncModel s here: @app . post ( ... ) async def some_path_endpoint ( request : fastapi . Request , item ): m = request . app . state . lib . get ( \"model_name\" ) result = await m . predict ( ... ) ... return ...","title":"Use the models"},{"location":"examples/tf_hub/","text":"In this tutorial we will learn how to load a pre-trained Tensorflow saved model We will use the Universal Sentence Encoder from tfhub as an example Download and extract \u00b6 Fisrt, download the file universal-sentence-encoder-multilingual_3.tar.gz and extract it in a asset_name/1 directory In this tutorial we will use /tmp/use/1 tar -xzvf universal-sentence-encoder-multilingual_3.tar.gz --directory /tmp/use/1 this will create the following tree: use \u2514\u2500\u2500 1 \u251c\u2500\u2500 assets \u251c\u2500\u2500 saved_model.pb \u2514\u2500\u2500 variables \u251c\u2500\u2500 variables.data-00000-of-00001 \u2514\u2500\u2500 variables.index Check the model configuration \u00b6 In order to use modelkit to use the model we have to check the model configuation in order to exctract outputs informations (key name, layer name, shape and type) and the inputs key name: import tensorflow as tf import tensorflow_text # module need by use model model = tf.saved_model.load(\"/tmp/use/1/\") print(model.signatures[\"serving_default\"].output_dtypes) print(model.signatures[\"serving_default\"].output_shapes) print(model.signatures[\"serving_default\"].outputs) print(model.signatures[\"serving_default\"].inputs[0]) # Output: # {'outputs': tf.float32} # {'outputs': TensorShape([None, 512])} # [<tf.Tensor 'Identity:0' shape=(None, 512) dtype=float32>] # <tf.Tensor 'inputs:0' shape=(None,) dtype=string> Quick load with modelkit \u00b6 We can now load the model by creating a TensorflowModel class and configuring it with information we just got from the model. Note that we have to declare a \"virtual\" asset \"asset\": \"use\" to directly set an asset path /tmp/use (without the 1 directory) import numpy as np import tensorflow_text import modelkit from modelkit.core.models.tensorflow_model import TensorflowModel class USEModel(TensorflowModel): CONFIGURATIONS = { \"use\": { \"asset\": \"use\", \"model_settings\": { \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"}, \"output_shapes\": {\"outputs\": (512,)}, \"output_dtypes\": {\"outputs\": np.float32}, \"asset_path\": \"/tmp/use\", }, } } and then we can test it using load_model model = modelkit.load_model(\"use\", models=USEModel) model.predict({\"inputs\": \"Hello world\"}) # note that the \"inputs\" keyword is extracted from the previous model configuration That's all ! We can start testing/using our model from sklearn.metrics.pairwise import cosine_distances sentence_1 = model.predict({\"inputs\": \"My dog is quite calm today\"})[\"outputs\"] sentence_2 = model.predict({\"inputs\": \"Mon chien est assez calme aujourd'hui\"})[\"outputs\"] sentence_3 = model.predict({\"inputs\": \"It rains on my house\"})[\"outputs\"] sentence_4 = model.predict({\"inputs\": \"Il pleut sur ma maison\"})[\"outputs\"] print(cosine_similarity([sentence_1, sentence_2, sentence_3, sentence_4])) # output : # [[1. 0.93083745 0.3172922 0.3379839 ] # [0.93083745 0.99999994 0.3522399 0.39009082] # [0.3172922 0.3522399 0.9999999 0.8444551 ] # [0.3379839 0.39009082 0.8444551 0.9999999 ]] # - sentence_1 close to sentence_2 (0.93) # - sentence_3 close to sentence_4 (0.84) # - other distances < 0.5 # => seems to work :-) Create an asset \u00b6 Once we have tested our model, we may want push and use it as an versioned asset modelkit assets new /tmp/use my_assets/use and then we can remove asset_path and add our asset name to our TensorflowModel import numpy as np import tensorflow_text import modelkit from modelkit.core.models.tensorflow_model import TensorflowModel class USEModel(TensorflowModel): CONFIGURATIONS = { \"use\": { \"asset\": \"my_assets/use:0.0\", \"model_settings\": { \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"}, \"output_shapes\": {\"outputs\": (512,)}, \"output_dtypes\": {\"outputs\": np.float32}, }, } } and then we may use a ModelLibrary to load it model_library = modelkit.ModelLibrary( required_models=[\"use\"], models=USEModel, ) model = model_library.get(\"use\") model.predict({\"inputs\": \"Hello world\"}) Using tensorflow serving \u00b6 Our model is directly compatible with our tensorflow-serving loading scripts Let's say we have saved our model in modelkit/use.py file, generate the configuation: modelkit tf-serving local-docker modelkit.use -r \"use\" it will create a config file ${MODELKIT_ASSETS_DIR}/config.config then we can start our tf-serving running docker run --name local-tf-serving -d -p 8500:8500 -p 8501:8501 -v ${MODELKIT_ASSETS_DIR}:/config -t tensorflow/serving --model_config_file=/config/config.config --rest_api_port=8501 --port=8500 then we can try use use our model with MODELKIT_TF_SERVING_ENABLE=1 , we should see the log line when loading the model [info ] Connecting to tensorflow serving mode=rest model_name=use port=8501 tf_serving_host=localhost","title":"Loading Model from TF Hub"},{"location":"examples/tf_hub/#download-and-extract","text":"Fisrt, download the file universal-sentence-encoder-multilingual_3.tar.gz and extract it in a asset_name/1 directory In this tutorial we will use /tmp/use/1 tar -xzvf universal-sentence-encoder-multilingual_3.tar.gz --directory /tmp/use/1 this will create the following tree: use \u2514\u2500\u2500 1 \u251c\u2500\u2500 assets \u251c\u2500\u2500 saved_model.pb \u2514\u2500\u2500 variables \u251c\u2500\u2500 variables.data-00000-of-00001 \u2514\u2500\u2500 variables.index","title":"Download and extract"},{"location":"examples/tf_hub/#check-the-model-configuration","text":"In order to use modelkit to use the model we have to check the model configuation in order to exctract outputs informations (key name, layer name, shape and type) and the inputs key name: import tensorflow as tf import tensorflow_text # module need by use model model = tf.saved_model.load(\"/tmp/use/1/\") print(model.signatures[\"serving_default\"].output_dtypes) print(model.signatures[\"serving_default\"].output_shapes) print(model.signatures[\"serving_default\"].outputs) print(model.signatures[\"serving_default\"].inputs[0]) # Output: # {'outputs': tf.float32} # {'outputs': TensorShape([None, 512])} # [<tf.Tensor 'Identity:0' shape=(None, 512) dtype=float32>] # <tf.Tensor 'inputs:0' shape=(None,) dtype=string>","title":"Check the model configuration"},{"location":"examples/tf_hub/#quick-load-with-modelkit","text":"We can now load the model by creating a TensorflowModel class and configuring it with information we just got from the model. Note that we have to declare a \"virtual\" asset \"asset\": \"use\" to directly set an asset path /tmp/use (without the 1 directory) import numpy as np import tensorflow_text import modelkit from modelkit.core.models.tensorflow_model import TensorflowModel class USEModel(TensorflowModel): CONFIGURATIONS = { \"use\": { \"asset\": \"use\", \"model_settings\": { \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"}, \"output_shapes\": {\"outputs\": (512,)}, \"output_dtypes\": {\"outputs\": np.float32}, \"asset_path\": \"/tmp/use\", }, } } and then we can test it using load_model model = modelkit.load_model(\"use\", models=USEModel) model.predict({\"inputs\": \"Hello world\"}) # note that the \"inputs\" keyword is extracted from the previous model configuration That's all ! We can start testing/using our model from sklearn.metrics.pairwise import cosine_distances sentence_1 = model.predict({\"inputs\": \"My dog is quite calm today\"})[\"outputs\"] sentence_2 = model.predict({\"inputs\": \"Mon chien est assez calme aujourd'hui\"})[\"outputs\"] sentence_3 = model.predict({\"inputs\": \"It rains on my house\"})[\"outputs\"] sentence_4 = model.predict({\"inputs\": \"Il pleut sur ma maison\"})[\"outputs\"] print(cosine_similarity([sentence_1, sentence_2, sentence_3, sentence_4])) # output : # [[1. 0.93083745 0.3172922 0.3379839 ] # [0.93083745 0.99999994 0.3522399 0.39009082] # [0.3172922 0.3522399 0.9999999 0.8444551 ] # [0.3379839 0.39009082 0.8444551 0.9999999 ]] # - sentence_1 close to sentence_2 (0.93) # - sentence_3 close to sentence_4 (0.84) # - other distances < 0.5 # => seems to work :-)","title":"Quick load with modelkit"},{"location":"examples/tf_hub/#create-an-asset","text":"Once we have tested our model, we may want push and use it as an versioned asset modelkit assets new /tmp/use my_assets/use and then we can remove asset_path and add our asset name to our TensorflowModel import numpy as np import tensorflow_text import modelkit from modelkit.core.models.tensorflow_model import TensorflowModel class USEModel(TensorflowModel): CONFIGURATIONS = { \"use\": { \"asset\": \"my_assets/use:0.0\", \"model_settings\": { \"output_tensor_mapping\": {\"outputs\": \"Identity:0\"}, \"output_shapes\": {\"outputs\": (512,)}, \"output_dtypes\": {\"outputs\": np.float32}, }, } } and then we may use a ModelLibrary to load it model_library = modelkit.ModelLibrary( required_models=[\"use\"], models=USEModel, ) model = model_library.get(\"use\") model.predict({\"inputs\": \"Hello world\"})","title":"Create an asset"},{"location":"examples/tf_hub/#using-tensorflow-serving","text":"Our model is directly compatible with our tensorflow-serving loading scripts Let's say we have saved our model in modelkit/use.py file, generate the configuation: modelkit tf-serving local-docker modelkit.use -r \"use\" it will create a config file ${MODELKIT_ASSETS_DIR}/config.config then we can start our tf-serving running docker run --name local-tf-serving -d -p 8500:8500 -p 8501:8501 -v ${MODELKIT_ASSETS_DIR}:/config -t tensorflow/serving --model_config_file=/config/config.config --rest_api_port=8501 --port=8500 then we can try use use our model with MODELKIT_TF_SERVING_ENABLE=1 , we should see the log line when loading the model [info ] Connecting to tensorflow serving mode=rest model_name=use port=8501 tf_serving_host=localhost","title":"Using tensorflow serving"},{"location":"examples/nlp_sentiment/advanced_models/","text":"You have now seen most development features and how to implement everything from the Tokenizer to the Classifier. Let's see how we can put it all together, while reviewing all Modelkit's features we have seen so far and introducing a powerful new feature: model dependencies. Composing models \u00b6 modelkit 's allows you to add models as dependencies of other models, and use them in the _predict methods. For example, it is desirable for our classifier to take string reviews as input, and output a class label (\"good\" or \"bad\"). This can be achieved using model dependencies. We need to specify the model_dependencies key in the CONFIGURATIONS map to add the Tokenizer and the Vectorizer as dependencies: import modelkit class Classifier ( modelkit . Model [ str , str ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" } }, } The modelkit.ModelLibrary will take care of loading the model dependencies before your model is available. They are then made available in the model_dependencies attribute, and can readily be used in the _predict method. For example: import modelkit class Classifier ( modelkit . Model [ str , str ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" } }, } ... def _predict_batch ( self , reviews ): # this looks like the previous end-to-end example from the previous section tokenized_reviews = self . model_dependencies [ \"imdb_tokenizer\" ] . predict_batch ( reviews ) vectorized_reviews = self . model_dependencies [ \"imdb_vectorizer\" ] . predict_batch ( tokenized_reviews , length = 64 ) predictions_scores = self . model . predict ( vectorized_reviews ) predictions_classes = [ \"good\" if score >= 0.5 else \"bad\" for score in predictions_scores ] return predictions_classes This end-to-end classifier is much easier to use, and still loadable easily: let us use a ModelLibrary : # define the model library with all necessary models model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) # this will load all models classifier = model_library . get ( \"imdb_classifier\" ) classifier . predict ( \"This movie is freaking awesome, I love the main character\" ) # good Adding complex output \u00b6 We may want to output the score as well as a class when using the model. Although we could return a dictionary, this can just as easily be achieved by outputing a Pydantic model, which offers more advanced validation features (and is used under the hood in modelkit ), and can readily be used in fastapi endpoints. This is really good practice, as it makes your code more understandable (even more so as your number of models grow). Let us modify our code to specify a pydantic.BaseModel as the output of our Model , which contains a label and the score. import modelkit import numpy as np import pydantic import tensorflow as tf class MovieSentimentItem ( pydantic . BaseModel ): label : str score : float class Classifier ( modelkit . Model [ str , MovieSentimentItem ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" }, }, } TEST_CASES = [ { \"item\" : { \"text\" : \"i love this film, it's the best I've ever seen\" }, \"result\" : { \"score\" : 0.8441019058227539 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.1625385582447052 , \"label\" : \"bad\" }, }, ] def _load ( self ): self . model = tf . keras . models . load_model ( self . asset_path ) self . tokenizer = self . model_dependencies [ \"imdb_tokenizer\" ] self . vectorizer = self . model_dependencies [ \"imdb_vectorizer\" ] def _predict_batch ( self , reviews ): texts = [ reviews . text for review in reviews ] tokenized_reviews = self . tokenizer . predict_batch ( texts ) vectorized_reviews = self . vectorizer . predict_batch ( tokenized_reviews , length = 64 ) predictions_scores = self . model . predict ( vectorized_reviews ) return [ { \"score\" : score , \"label\" : \"good\" if score >= 0.5 else \"bad\" } for score in predictions_scores ] We also added some TEST_CASES to make sure that our model still behaves correctly. Note Although we return a dictionary, it will be turned into a MovieSentimentItem , and validated by Pydantic. We can now test the Model : model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) classifier = model_library . get ( \"imdb_classifier\" ) classifier . test () prediction = classifier . predict ({ \"text\" : \"I love the main character\" }) print ( prediction ) # MovieSentimentItem(label='good', score=0.6801363825798035) print ( prediction . label ) # good print ( prediction . score ) # 0.6801363825798035 Multiple Model configurations \u00b6 To conclude this tutorial, let us briefly see how to define multiple configurations, and why one would want to do that. Assume that our Classifier model goes to production and most users are happy with it, but some are not. You start from scratch: redefine a tokenizer, vectorizer, train a new classifier with a different architecture and more data, and save it preciously. Since you have always been the original guy in the room, all the new models now have a \"_SOTA\" suffix. You managed to keep the same inputs, outputs, pipeline architecture and made your process reproductible. \"That should do it !\", you yell across the open space. However, you probably do not want to surprise your clients with a new model without informing them before.Some might want to stick with the old one, while the unhappy ones would want to change ASAP. Modelkit has got your back, and allows you to define multiple configurations while keeping the exact same code. Here is how you go about doing this: import modelkit class Classifier ( modelkit . Model [ MovieReviewItem , MovieSentimentItem ]): CONFIGURATIONS = { \"classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"tokenizer\" : \"imdb_tokenizer\" , \"vectorizer\" : \"imdb_vectorizer\" , }, \"test_cases\" : [ { \"item\" : { \"text\" : \"i love this film, it's the best i've ever seen\" }, \"result\" : { \"score\" : 0.8441019058227539 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.1625385582447052 , \"label\" : \"bad\" }, }, ], }, \"classifier_SOTA\" : { \"asset\" : \"imdb_model_SOTA.h5\" , \"model_dependencies\" : { \"tokenizer\" : \"imdb_tokenizer_SOTA\" , \"vectorizer\" : \"imdb_vectorizer_SOTA\" , }, \"test_cases\" : [ { \"item\" : { \"text\" : \"i love this film, it's the best i've ever seen\" }, \"result\" : { \"score\" : 1.0 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.0 , \"label\" : \"bad\" }, }, ], }, } ... In order to use the same code within _predict , we have renamed the dependencies, by using a dictionary instead of a set in model_dependencies . The model_dependencies attribute of classifier and classifier_SOTA will have the same tokenizer and vectorizer keys, but pointing to different Model s. Also, the tests_cases are now part of each individual configuration so that to test each one independently. Now both of your models are available through the same library: model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) classifier_deprecated = model_library . get ( \"classifier\" ) classifier_SOTA = model_library . get ( \"classifier_SOTA\" ) Additionally, you can decide to filter out which one you are interested in (and avoid it being loaded in memory if it is unused), by specifying the required_models keyword argument: model_library = modelkit . ModelLibrary ( required_models = [ \"classifier_SOTA\" ] models = [ Tokenizer , Vectorizer , Classifier ] ) As you can see, the CONFIGURATIONS map and the dependency renaming helped make this task easier than we may have thought in the first instance.","title":"Advanced models"},{"location":"examples/nlp_sentiment/advanced_models/#composing-models","text":"modelkit 's allows you to add models as dependencies of other models, and use them in the _predict methods. For example, it is desirable for our classifier to take string reviews as input, and output a class label (\"good\" or \"bad\"). This can be achieved using model dependencies. We need to specify the model_dependencies key in the CONFIGURATIONS map to add the Tokenizer and the Vectorizer as dependencies: import modelkit class Classifier ( modelkit . Model [ str , str ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" } }, } The modelkit.ModelLibrary will take care of loading the model dependencies before your model is available. They are then made available in the model_dependencies attribute, and can readily be used in the _predict method. For example: import modelkit class Classifier ( modelkit . Model [ str , str ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" } }, } ... def _predict_batch ( self , reviews ): # this looks like the previous end-to-end example from the previous section tokenized_reviews = self . model_dependencies [ \"imdb_tokenizer\" ] . predict_batch ( reviews ) vectorized_reviews = self . model_dependencies [ \"imdb_vectorizer\" ] . predict_batch ( tokenized_reviews , length = 64 ) predictions_scores = self . model . predict ( vectorized_reviews ) predictions_classes = [ \"good\" if score >= 0.5 else \"bad\" for score in predictions_scores ] return predictions_classes This end-to-end classifier is much easier to use, and still loadable easily: let us use a ModelLibrary : # define the model library with all necessary models model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) # this will load all models classifier = model_library . get ( \"imdb_classifier\" ) classifier . predict ( \"This movie is freaking awesome, I love the main character\" ) # good","title":"Composing models"},{"location":"examples/nlp_sentiment/advanced_models/#adding-complex-output","text":"We may want to output the score as well as a class when using the model. Although we could return a dictionary, this can just as easily be achieved by outputing a Pydantic model, which offers more advanced validation features (and is used under the hood in modelkit ), and can readily be used in fastapi endpoints. This is really good practice, as it makes your code more understandable (even more so as your number of models grow). Let us modify our code to specify a pydantic.BaseModel as the output of our Model , which contains a label and the score. import modelkit import numpy as np import pydantic import tensorflow as tf class MovieSentimentItem ( pydantic . BaseModel ): label : str score : float class Classifier ( modelkit . Model [ str , MovieSentimentItem ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"imdb_tokenizer\" , \"imdb_vectorizer\" }, }, } TEST_CASES = [ { \"item\" : { \"text\" : \"i love this film, it's the best I've ever seen\" }, \"result\" : { \"score\" : 0.8441019058227539 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.1625385582447052 , \"label\" : \"bad\" }, }, ] def _load ( self ): self . model = tf . keras . models . load_model ( self . asset_path ) self . tokenizer = self . model_dependencies [ \"imdb_tokenizer\" ] self . vectorizer = self . model_dependencies [ \"imdb_vectorizer\" ] def _predict_batch ( self , reviews ): texts = [ reviews . text for review in reviews ] tokenized_reviews = self . tokenizer . predict_batch ( texts ) vectorized_reviews = self . vectorizer . predict_batch ( tokenized_reviews , length = 64 ) predictions_scores = self . model . predict ( vectorized_reviews ) return [ { \"score\" : score , \"label\" : \"good\" if score >= 0.5 else \"bad\" } for score in predictions_scores ] We also added some TEST_CASES to make sure that our model still behaves correctly. Note Although we return a dictionary, it will be turned into a MovieSentimentItem , and validated by Pydantic. We can now test the Model : model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) classifier = model_library . get ( \"imdb_classifier\" ) classifier . test () prediction = classifier . predict ({ \"text\" : \"I love the main character\" }) print ( prediction ) # MovieSentimentItem(label='good', score=0.6801363825798035) print ( prediction . label ) # good print ( prediction . score ) # 0.6801363825798035","title":"Adding complex output"},{"location":"examples/nlp_sentiment/advanced_models/#multiple-model-configurations","text":"To conclude this tutorial, let us briefly see how to define multiple configurations, and why one would want to do that. Assume that our Classifier model goes to production and most users are happy with it, but some are not. You start from scratch: redefine a tokenizer, vectorizer, train a new classifier with a different architecture and more data, and save it preciously. Since you have always been the original guy in the room, all the new models now have a \"_SOTA\" suffix. You managed to keep the same inputs, outputs, pipeline architecture and made your process reproductible. \"That should do it !\", you yell across the open space. However, you probably do not want to surprise your clients with a new model without informing them before.Some might want to stick with the old one, while the unhappy ones would want to change ASAP. Modelkit has got your back, and allows you to define multiple configurations while keeping the exact same code. Here is how you go about doing this: import modelkit class Classifier ( modelkit . Model [ MovieReviewItem , MovieSentimentItem ]): CONFIGURATIONS = { \"classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"tokenizer\" : \"imdb_tokenizer\" , \"vectorizer\" : \"imdb_vectorizer\" , }, \"test_cases\" : [ { \"item\" : { \"text\" : \"i love this film, it's the best i've ever seen\" }, \"result\" : { \"score\" : 0.8441019058227539 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.1625385582447052 , \"label\" : \"bad\" }, }, ], }, \"classifier_SOTA\" : { \"asset\" : \"imdb_model_SOTA.h5\" , \"model_dependencies\" : { \"tokenizer\" : \"imdb_tokenizer_SOTA\" , \"vectorizer\" : \"imdb_vectorizer_SOTA\" , }, \"test_cases\" : [ { \"item\" : { \"text\" : \"i love this film, it's the best i've ever seen\" }, \"result\" : { \"score\" : 1.0 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.0 , \"label\" : \"bad\" }, }, ], }, } ... In order to use the same code within _predict , we have renamed the dependencies, by using a dictionary instead of a set in model_dependencies . The model_dependencies attribute of classifier and classifier_SOTA will have the same tokenizer and vectorizer keys, but pointing to different Model s. Also, the tests_cases are now part of each individual configuration so that to test each one independently. Now both of your models are available through the same library: model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) classifier_deprecated = model_library . get ( \"classifier\" ) classifier_SOTA = model_library . get ( \"classifier_SOTA\" ) Additionally, you can decide to filter out which one you are interested in (and avoid it being loaded in memory if it is unused), by specifying the required_models keyword argument: model_library = modelkit . ModelLibrary ( required_models = [ \"classifier_SOTA\" ] models = [ Tokenizer , Vectorizer , Classifier ] ) As you can see, the CONFIGURATIONS map and the dependency renaming helped make this task easier than we may have thought in the first instance.","title":"Multiple Model configurations"},{"location":"examples/nlp_sentiment/classifier/","text":"In this section, we will implement a sentiment classifier with Keras, using the components we have been developping so far. Data preparation \u00b6 We will reuse the read_dataset function, in addition to a few other helper functions which will continue taking advantage of generators to avoid loading up the entire dataset into memory: alternate(f, g) : to yield items alternatively between the f and g generators alternate_labels() : to yield 1 and 0, alternatively, in sync with the previous alternate function import glob import os from typing import Generator , Any def read_dataset ( path : str ) -> Generator [ str , None , None ]: for review in glob . glob ( os . path . join ( path , \"*.txt\" )): with open ( review , 'r' , encoding = 'utf-8' ) as f : yield f . read () def alternate ( f : Generator [ Any , Any , Any ], g : Generator [ Any , Any , Any ] ) -> Generator [ Any , None , None ]: while True : try : yield next ( f ) yield next ( g ) except StopIteration : break def alternate_labels () -> Generator [ int , None , None ]: while True : yield 1 yield 0 Let's plug these pipes together so that to efficiently read and process the IMDB reviews dataset for our next Keras classifier, leveraging the Tokenizer and Vectorizer we implemented in the first two sections: def process ( path , tokenizer , vectorizer , length , batch_size ): # read the positive sentiment reviews dataset positive_dataset = read_dataset ( os . path . join ( path , \"pos\" )) # read the negative sentiment reviews dataset negative_dataset = read_dataset ( os . path . join ( path , \"neg\" )) # alternate between positives and negatives examples dataset = alternate ( positive_dataset , negative_dataset ) # generate labels in sync with the previous generator: 1 for positive examples, 0 for negative ones labels = alternate_labels () # tokenize the reviews using our Tokenizer Model tokenized_dataset = tokenizer . predict_gen ( dataset , batch_size = batch_size ) # vectorize the reviews using our Vectorizer Model vectorized_dataset = vectorizer . predict_gen ( tokenized_dataset , length = length , batch_size = batch_size ) # yield (review, label) tuples for Keras yield from zip ( vectorized_dataset , labels ) Let's try it out on the first examples: i = 0 for review , label in process ( os . path . join ( \"aclImdb\" , \"train\" ), Tokenizer (), Vectorizer (), length = 64 , batch_size = 64 , ): print ( review , label ) i += 1 if i >= 10 : break Model library \u00b6 So far, we have instantiated Tokenizer and Vectorizer classes as standard objects. Modelkit provides a simpler and more powerful way to instantiate Models , the library: modelkit.ModelLibrary . The purpose of the ModelLibrary is to have a single way to load any of the models that are defined, any way you decide to keep them organized. With a library ModelLibrary you can fetch models by their configuration key using ModelLibrary.get while ensuring that models are only loaded once use prediction caching: Prediction Caching use lazy loading for models: Lazy Loading override model parameters: Settings Although we will not cover all of these features here, let's see how we can take advantage of the ModelLibrary with our previous work. Let us define a model library, it can take the clases of models as input: import modelkit ... # define Vectorizer and Tokenizer classes model_library = modelkit . ModelLibrary ( models = [ Vectorizer , Tokenizer ]) tokenizer = model_library . get ( \"imdb_tokenizer\" ) vectorizer = model_library . get ( \"imdb_vectorizer\" ) Or, alternatively use the modules they are present in: import module_with_models model_library = modelkit . ModelLibrary ( models = module_with_models ) This method is the preferred method, because it encourages you to adopt a package-like organisation of your Models (see Organizing Models ) Using models to create a TF.Dataset \u00b6 Now that we know how to reach out models, let us use them to create a TF Dataset from our data processing generators: import os import tensorflow as tf BATCH_SIZE = 64 LENGTH = 64 training_set = ( tf . data . Dataset . from_generator ( lambda : process ( os . path . join ( \"aclImdb\" , \"train\" ), tokenizer , vectorizer , length = LENGTH , batch_size = BATCH_SIZE , ), output_types = ( tf . int16 , tf . int16 ), ) . batch ( BATCH_SIZE ) . repeat () . prefetch ( 1 ) ) validation_set = ( tf . data . Dataset . from_generator ( lambda : process ( os . path . join ( \"aclImdb\" , \"test\" ), tokenizer , vectorizer , length = LENGTH , batch_size = BATCH_SIZE , ), output_types = ( tf . int16 , tf . int16 ), ) . batch ( BATCH_SIZE ) . repeat () . prefetch ( 1 ) ) Training a Keras model \u00b6 Let's train a basic Keras classifier to predict whether an IMDB review is positive or negative, and save it to disk. import tensorflow as tf model = tf . keras . Sequential ( [ tf . keras . layers . Embedding ( input_dim = len ( vectorizer . vocabulary ) + 2 , output_dim = 64 , input_length = LENGTH ), tf . keras . layers . Lambda ( lambda x : tf . reduce_sum ( x , axis = 1 )), tf . keras . layers . Dense ( 1 , activation = \"sigmoid\" ), ] ) model . compile ( tf . keras . optimizers . Adam ( 0.001 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . binary_accuracy ], ) model . build () model . fit ( training_set , validation_data = validation_set , epochs = 10 , steps_per_epoch = 100 , validation_steps = 10 , ) model . save ( \"imdb_model.h5\" , include_optimizer = False , save_format = \"h5\" , save_traces = False ) Classifier Model \u00b6 Voil\u00e0 ! As we already did for the Vectorizer, we will embed the just-saved imdb_model.h5 in a basic Modelkit Model which we will further upgrade in the next section. import modelkit import tensorflow as tf from typing import List class Classifier ( modelkit . Model [ List [ int ], float ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" }} def _load ( self ): self . model = tf . keras . models . load_model ( self . asset_path ) def _predict_batch ( self , vectorized_reviews ): return self . model . predict ( vectorized_reviews ) Much like we did for the Vectorizer , the Classifier model has a imdb_classifier configuration with an asset pointing to the imdb_model.h5 . We also benefit from Keras' predict ability to batch predictions in our _predict_batch method. End-to-end prediction \u00b6 To summarize, here is how we would want to chain our Tokenizer , Vectorizer and Classifier : import modelkit library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) tokenizer = library . get ( \"imdb_tokenizer\" ) vectorizer = library . get ( \"imdb_vectorizer\" ) classifier = library . get ( \"imdb_classifier\" ) review = \"I freaking love this movie, the main character is so cool !\" tokenized_review = tokenizer ( review ) # or tokenizer.predict vectorized_review = vectorizer ( tokenized_review ) # or vectorizer.predict prediction = classifier ( vectorized_review ) # or classifier.predict In the next (and final) section, we will see how modelkit can be used to perform this operation in a single Model .","title":"Classifier"},{"location":"examples/nlp_sentiment/classifier/#data-preparation","text":"We will reuse the read_dataset function, in addition to a few other helper functions which will continue taking advantage of generators to avoid loading up the entire dataset into memory: alternate(f, g) : to yield items alternatively between the f and g generators alternate_labels() : to yield 1 and 0, alternatively, in sync with the previous alternate function import glob import os from typing import Generator , Any def read_dataset ( path : str ) -> Generator [ str , None , None ]: for review in glob . glob ( os . path . join ( path , \"*.txt\" )): with open ( review , 'r' , encoding = 'utf-8' ) as f : yield f . read () def alternate ( f : Generator [ Any , Any , Any ], g : Generator [ Any , Any , Any ] ) -> Generator [ Any , None , None ]: while True : try : yield next ( f ) yield next ( g ) except StopIteration : break def alternate_labels () -> Generator [ int , None , None ]: while True : yield 1 yield 0 Let's plug these pipes together so that to efficiently read and process the IMDB reviews dataset for our next Keras classifier, leveraging the Tokenizer and Vectorizer we implemented in the first two sections: def process ( path , tokenizer , vectorizer , length , batch_size ): # read the positive sentiment reviews dataset positive_dataset = read_dataset ( os . path . join ( path , \"pos\" )) # read the negative sentiment reviews dataset negative_dataset = read_dataset ( os . path . join ( path , \"neg\" )) # alternate between positives and negatives examples dataset = alternate ( positive_dataset , negative_dataset ) # generate labels in sync with the previous generator: 1 for positive examples, 0 for negative ones labels = alternate_labels () # tokenize the reviews using our Tokenizer Model tokenized_dataset = tokenizer . predict_gen ( dataset , batch_size = batch_size ) # vectorize the reviews using our Vectorizer Model vectorized_dataset = vectorizer . predict_gen ( tokenized_dataset , length = length , batch_size = batch_size ) # yield (review, label) tuples for Keras yield from zip ( vectorized_dataset , labels ) Let's try it out on the first examples: i = 0 for review , label in process ( os . path . join ( \"aclImdb\" , \"train\" ), Tokenizer (), Vectorizer (), length = 64 , batch_size = 64 , ): print ( review , label ) i += 1 if i >= 10 : break","title":"Data preparation"},{"location":"examples/nlp_sentiment/classifier/#model-library","text":"So far, we have instantiated Tokenizer and Vectorizer classes as standard objects. Modelkit provides a simpler and more powerful way to instantiate Models , the library: modelkit.ModelLibrary . The purpose of the ModelLibrary is to have a single way to load any of the models that are defined, any way you decide to keep them organized. With a library ModelLibrary you can fetch models by their configuration key using ModelLibrary.get while ensuring that models are only loaded once use prediction caching: Prediction Caching use lazy loading for models: Lazy Loading override model parameters: Settings Although we will not cover all of these features here, let's see how we can take advantage of the ModelLibrary with our previous work. Let us define a model library, it can take the clases of models as input: import modelkit ... # define Vectorizer and Tokenizer classes model_library = modelkit . ModelLibrary ( models = [ Vectorizer , Tokenizer ]) tokenizer = model_library . get ( \"imdb_tokenizer\" ) vectorizer = model_library . get ( \"imdb_vectorizer\" ) Or, alternatively use the modules they are present in: import module_with_models model_library = modelkit . ModelLibrary ( models = module_with_models ) This method is the preferred method, because it encourages you to adopt a package-like organisation of your Models (see Organizing Models )","title":"Model library"},{"location":"examples/nlp_sentiment/classifier/#using-models-to-create-a-tfdataset","text":"Now that we know how to reach out models, let us use them to create a TF Dataset from our data processing generators: import os import tensorflow as tf BATCH_SIZE = 64 LENGTH = 64 training_set = ( tf . data . Dataset . from_generator ( lambda : process ( os . path . join ( \"aclImdb\" , \"train\" ), tokenizer , vectorizer , length = LENGTH , batch_size = BATCH_SIZE , ), output_types = ( tf . int16 , tf . int16 ), ) . batch ( BATCH_SIZE ) . repeat () . prefetch ( 1 ) ) validation_set = ( tf . data . Dataset . from_generator ( lambda : process ( os . path . join ( \"aclImdb\" , \"test\" ), tokenizer , vectorizer , length = LENGTH , batch_size = BATCH_SIZE , ), output_types = ( tf . int16 , tf . int16 ), ) . batch ( BATCH_SIZE ) . repeat () . prefetch ( 1 ) )","title":"Using models to create a TF.Dataset"},{"location":"examples/nlp_sentiment/classifier/#training-a-keras-model","text":"Let's train a basic Keras classifier to predict whether an IMDB review is positive or negative, and save it to disk. import tensorflow as tf model = tf . keras . Sequential ( [ tf . keras . layers . Embedding ( input_dim = len ( vectorizer . vocabulary ) + 2 , output_dim = 64 , input_length = LENGTH ), tf . keras . layers . Lambda ( lambda x : tf . reduce_sum ( x , axis = 1 )), tf . keras . layers . Dense ( 1 , activation = \"sigmoid\" ), ] ) model . compile ( tf . keras . optimizers . Adam ( 0.001 ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = [ tf . keras . metrics . binary_accuracy ], ) model . build () model . fit ( training_set , validation_data = validation_set , epochs = 10 , steps_per_epoch = 100 , validation_steps = 10 , ) model . save ( \"imdb_model.h5\" , include_optimizer = False , save_format = \"h5\" , save_traces = False )","title":"Training a Keras model"},{"location":"examples/nlp_sentiment/classifier/#classifier-model","text":"Voil\u00e0 ! As we already did for the Vectorizer, we will embed the just-saved imdb_model.h5 in a basic Modelkit Model which we will further upgrade in the next section. import modelkit import tensorflow as tf from typing import List class Classifier ( modelkit . Model [ List [ int ], float ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" }} def _load ( self ): self . model = tf . keras . models . load_model ( self . asset_path ) def _predict_batch ( self , vectorized_reviews ): return self . model . predict ( vectorized_reviews ) Much like we did for the Vectorizer , the Classifier model has a imdb_classifier configuration with an asset pointing to the imdb_model.h5 . We also benefit from Keras' predict ability to batch predictions in our _predict_batch method.","title":"Classifier Model"},{"location":"examples/nlp_sentiment/classifier/#end-to-end-prediction","text":"To summarize, here is how we would want to chain our Tokenizer , Vectorizer and Classifier : import modelkit library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) tokenizer = library . get ( \"imdb_tokenizer\" ) vectorizer = library . get ( \"imdb_vectorizer\" ) classifier = library . get ( \"imdb_classifier\" ) review = \"I freaking love this movie, the main character is so cool !\" tokenized_review = tokenizer ( review ) # or tokenizer.predict vectorized_review = vectorizer ( tokenized_review ) # or vectorizer.predict prediction = classifier ( vectorized_review ) # or classifier.predict In the next (and final) section, we will see how modelkit can be used to perform this operation in a single Model .","title":"End-to-end prediction"},{"location":"examples/nlp_sentiment/intro/","text":"In this tutorial, you will learn how to leverage Modelkit as a support for a common NLP task: Sentiment Analysis. You will see how Modelkit can be useful as you progress along the following steps: Implementing the Tokenizer leveraging spaCy Implementing the Vectorizer leveraging Scikit-Learn Building a simple Classifier leveraging Keras to predict whether a review is negative or positive Last but not least, you will get to see how a breaze it is to speed up your go-to-production process with Modelkit.","title":"Intro"},{"location":"examples/nlp_sentiment/tldr/","text":"Here is the entire tutorial implementation, covering most Modelkit's features to get you started. from typing import List , Optional import modelkit import numpy as np import pydantic import spacy import tensorflow as tf class Tokenizer ( modelkit . Model [ str , List [ str ]]): CONFIGURATIONS = { \"imdb_tokenizer\" : {}} TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" ], }, ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} TEST_CASES = [ { \"item\" : [], \"result\" : []}, { \"item\" : [], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 0 ] * 10 }, { \"item\" : [ \"movie\" ], \"result\" : [ 888 ]}, { \"item\" : [ \"unknown_token\" ], \"result\" : []}, { \"item\" : [ \"unknown_token\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 1 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"result\" : [ 888 , 1156 ]}, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 888 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 , \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, ] def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i + 2 self . _vectorizer = np . vectorize ( lambda x : self . vocabulary . get ( x , 1 )) def _predict ( self , tokens , length = None , drop_oov = True ): vectorized = ( np . array ( self . _vectorizer ( tokens ), dtype = np . int ) if tokens else np . array ([], dtype = int ) ) if drop_oov and len ( vectorized ): vectorized = np . delete ( vectorized , vectorized == 1 ) if not length : return vectorized . tolist () result = np . zeros ( length ) vectorized = vectorized [: length ] result [: len ( vectorized )] = vectorized return result . tolist () class MovieReviewItem ( pydantic . BaseModel ): text : str rating : Optional [ float ] = None # could be useful in the future ? but not mandatory class MovieSentimentItem ( pydantic . BaseModel ): label : str score : float class Classifier ( modelkit . Model [ MovieReviewItem , MovieSentimentItem ]): CONFIGURATIONS = { \"imdb_classifier\" : { \"asset\" : \"imdb_model.h5\" , \"model_dependencies\" : { \"tokenizer\" : \"imdb_tokenizer\" , \"vectorizer\" : \"imdb_vectorizer\" , }, }, } TEST_CASES = [ { \"item\" : { \"text\" : \"i love this film, it's the best I've ever seen\" }, \"result\" : { \"score\" : 0.8441019058227539 , \"label\" : \"good\" }, }, { \"item\" : { \"text\" : \"this movie sucks, it's the worst I have ever seen\" }, \"result\" : { \"score\" : 0.1625385582447052 , \"label\" : \"bad\" }, }, ] def _load ( self ): self . model = tf . keras . models . load_model ( self . asset_path ) self . tokenizer = self . model_dependencies [ \"tokenizer\" ] self . vectorizer = self . model_dependencies [ \"vectorizer\" ] def _predict_batch ( self , reviews ): texts = [ review . text for review in reviews ] tokenized_reviews = self . tokenizer . predict_batch ( texts ) vectorized_reviews = self . vectorizer . predict_batch ( tokenized_reviews , length = 64 ) predictions_scores = self . model . predict ( vectorized_reviews ) predictions = [ { \"score\" : score , \"label\" : \"good\" if score >= 0.5 else \"bad\" } for score in predictions_scores ] return predictions model_library = modelkit . ModelLibrary ( models = [ Tokenizer , Vectorizer , Classifier ]) classifier = model_library . get ( \"imdb_classifier\" ) prediction = classifier . predict ({ \"text\" : \"I love the main character\" }) print ( prediction . label )","title":"TL;DR"},{"location":"examples/nlp_sentiment/tokenizer/","text":"In this section, we will cover the basics of modelkit 's API, and use spaCy as tokenizer for our NLP pipeline. Installation \u00b6 Once you have set up a fresh python environment, let's install modelkit , spacy and grab the small english model. pip install modelkit spacy python -m spacy download en_core_web_sm Simple Model predict \u00b6 To define a modelkit Model , you need to: create class inheriting from modelkit.Model implement a _predict method To begin with, let's create a minimal tokenizer: import modelkit class Tokenizer ( modelkit . Model ): def _predict ( self , text ): return text . split () That's it! It is very minimal, but sufficient to define a modelkit Model . You can now instantiate and call the Model : tokenizer = Tokenizer () tokenizer . predict ( \"I am a Data Scientist from Amiens, France\" ) Other ways to call predict It is also possible to get predictions for batches (lists of items): tokenizer . predict_batch ([ \"I am a Data Scientist from Amiens, France\" , \"And I use modelkit\" ]) or call predict as a generator: for prediction in tokenizer . predict_gen (( \"I am a Data Scientist from Amiens, France\" ,)): print ( prediction ) Complex Model initialization \u00b6 Let's now use spaCy to get closer to a production-ready tokenizer. This will also help demonstrate additional Modelkit features. import modelkit import spacy class Tokenizer ( modelkit . Model ): def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict ( self , text ): text = \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) return [ t . lower_ for t in self . nlp ( text ) # self.nlp is guaranteed to be initialized if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] We implement a _load method, which is where any asset, artifact, and other complex model attributes are created. This method will be called exactly once in the lifetime of the Model object. We define the spacy pipeline in the _load method (as opposed to _predict or the __init__ methods), because it allows your model to benefit from advanced modelkit features such as lazy loading and dependency management . Since we will only be using the tokenizer and not the many other cool spacy features, let's not forget to disable them. We can instantiate the model and get predictions as before: tokenizer = Tokenizer () # _load is called tokenizer . predict ( \"spaCy is a great lib for NLP \ud83d\ude00\" ) # ['spacy', 'great', 'lib', 'nlp'] Batch computation \u00b6 So far, we have only implemented the _predict method, which tokenizes items one by one. In many instances, however, models will be called with many items at once, and we can leverage vectorization for speedups. This is particularly true when using other frameworks (Numpy, spaCy, Tensorflow, PyTorch etc.), or distant calls (TF Serving, database accesses etc.). To leverage batching, modelkit allows you to define a _predict_batch method to process lists of items, and thus kill multiple birds with one stone . Here we use spaCy's pipe method to tokenize items in batch: import modelkit import spacy class Tokenizer ( modelkit . Model ): def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] Compared to the implementation with a _predict call, the time needed to tokenize batches of data is divided by 2. For example, using ipython's timeit to process a list of a 100 strings: %timeit [Tokenizer().predict(\"spaCy is a great lib for NLP\") for _ in range(100)] # 11.1 ms \u00b1 203 \u00b5s per loop on a 2020 Macbook Pro. %timeit Tokenizer().predict_batch([\"spaCy is a great lib for NLP] * 100, batch_size=64) # 5.5 ms \u00b1 105 \u00b5s per loop on a 2020 Macbook Pro. Caching predictions modelkit also allows you to use prediction caching (using Redis, or Python native caches) to improve computation times when the same items are seen over and over Additional features \u00b6 Tests \u00b6 So far the tokenizer is relatively simple, but it is always useful to test your code. modelkit encourages you to add test cases alongside the Model class definition to ensure that it behaves as intended, and serve as documentation. import modelkit import spacy class Tokenizer ( modelkit . Model ): TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" , \"failing\" , \"test\" ], }, # fails as intended ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] You can run these test cases in the interactive programming tool of your choice (e.g. ipython , jupyter etc.) using the test method: Tokenizer () . test () # TEST 1: SUCCESS # TEST 2: SUCCESS # TEST 3: SUCCESS # TEST 4: SUCCESS # TEST 5: FAILED test failed on item # item = '<br/>a b c data<br/> e scientist' # expected = list instance # result = list instance Run using pytest It is also possible to automatically test all models using the pytest integration, using the Modelkit autotesting fixture . Woops, seems like we need to fix the last test! Input and output specification \u00b6 It is good practice to specify inputs and outputs of models in production code This allows calls to be validated, thus ensuring consistency between calls, dependencies, services, and raising alerts when Models are not called as expected. This is also good for documentation, to understand how to use a given model, and during development to benefit from static type checking (e.g. with mypy ). modelkit allows you to define the expected input and output types of your model by subclassing Model[input_type, output_type] , where input_type and output_type can be standard Python types, dataclasses, or complex pydantic models. Let's add specification our Tokenizer to conclude this first part: from typing import List import modelkit import spacy class Tokenizer ( modelkit . Model [ str , List [ str ]]): TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" , \"failing\" , \"test\" ], }, # fails as intended ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] Calling the model with an unexpected type will raise a Modelkit ItemValidationException : Tokenizer () . predict ([ 1 , 2 , 3 , 4 ]) And mypy will raise errors if it encounters calls that are not correct: result : int = Tokenizer () . predict ( \"some text\" ) Conclusion \u00b6 That's it! In this modelkit introduction, you have learned: How to create a basic Model by inheriting from modelkit.Model and implementing a _predict method How to correctly load artefacts/assets by overriding the _load method How to leverage batch computing to speed up execution by implementing a _predict_batch method How to add tests to ensure everything works as intended using TEST_CASES right in your model definition How to add specification to your model's inputs and outputs using modelkit.Model[input_type, output_type] Final Tokenizer code \u00b6 from typing import List import modelkit import spacy class Tokenizer ( modelkit . Model [ str , List [ str ]]): CONFIGURATIONS = { \"imdb_tokenizer\" : {}} TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" ], }, ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] As you may have seen, there is a CONFIGURATIONS map in the class definition, we will cover it in the next section.","title":"Tokenizer"},{"location":"examples/nlp_sentiment/tokenizer/#installation","text":"Once you have set up a fresh python environment, let's install modelkit , spacy and grab the small english model. pip install modelkit spacy python -m spacy download en_core_web_sm","title":"Installation"},{"location":"examples/nlp_sentiment/tokenizer/#simple-model-predict","text":"To define a modelkit Model , you need to: create class inheriting from modelkit.Model implement a _predict method To begin with, let's create a minimal tokenizer: import modelkit class Tokenizer ( modelkit . Model ): def _predict ( self , text ): return text . split () That's it! It is very minimal, but sufficient to define a modelkit Model . You can now instantiate and call the Model : tokenizer = Tokenizer () tokenizer . predict ( \"I am a Data Scientist from Amiens, France\" ) Other ways to call predict It is also possible to get predictions for batches (lists of items): tokenizer . predict_batch ([ \"I am a Data Scientist from Amiens, France\" , \"And I use modelkit\" ]) or call predict as a generator: for prediction in tokenizer . predict_gen (( \"I am a Data Scientist from Amiens, France\" ,)): print ( prediction )","title":"Simple Model predict"},{"location":"examples/nlp_sentiment/tokenizer/#complex-model-initialization","text":"Let's now use spaCy to get closer to a production-ready tokenizer. This will also help demonstrate additional Modelkit features. import modelkit import spacy class Tokenizer ( modelkit . Model ): def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict ( self , text ): text = \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) return [ t . lower_ for t in self . nlp ( text ) # self.nlp is guaranteed to be initialized if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] We implement a _load method, which is where any asset, artifact, and other complex model attributes are created. This method will be called exactly once in the lifetime of the Model object. We define the spacy pipeline in the _load method (as opposed to _predict or the __init__ methods), because it allows your model to benefit from advanced modelkit features such as lazy loading and dependency management . Since we will only be using the tokenizer and not the many other cool spacy features, let's not forget to disable them. We can instantiate the model and get predictions as before: tokenizer = Tokenizer () # _load is called tokenizer . predict ( \"spaCy is a great lib for NLP \ud83d\ude00\" ) # ['spacy', 'great', 'lib', 'nlp']","title":"Complex Model initialization"},{"location":"examples/nlp_sentiment/tokenizer/#batch-computation","text":"So far, we have only implemented the _predict method, which tokenizes items one by one. In many instances, however, models will be called with many items at once, and we can leverage vectorization for speedups. This is particularly true when using other frameworks (Numpy, spaCy, Tensorflow, PyTorch etc.), or distant calls (TF Serving, database accesses etc.). To leverage batching, modelkit allows you to define a _predict_batch method to process lists of items, and thus kill multiple birds with one stone . Here we use spaCy's pipe method to tokenize items in batch: import modelkit import spacy class Tokenizer ( modelkit . Model ): def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] Compared to the implementation with a _predict call, the time needed to tokenize batches of data is divided by 2. For example, using ipython's timeit to process a list of a 100 strings: %timeit [Tokenizer().predict(\"spaCy is a great lib for NLP\") for _ in range(100)] # 11.1 ms \u00b1 203 \u00b5s per loop on a 2020 Macbook Pro. %timeit Tokenizer().predict_batch([\"spaCy is a great lib for NLP] * 100, batch_size=64) # 5.5 ms \u00b1 105 \u00b5s per loop on a 2020 Macbook Pro. Caching predictions modelkit also allows you to use prediction caching (using Redis, or Python native caches) to improve computation times when the same items are seen over and over","title":"Batch computation"},{"location":"examples/nlp_sentiment/tokenizer/#additional-features","text":"","title":"Additional features"},{"location":"examples/nlp_sentiment/tokenizer/#tests","text":"So far the tokenizer is relatively simple, but it is always useful to test your code. modelkit encourages you to add test cases alongside the Model class definition to ensure that it behaves as intended, and serve as documentation. import modelkit import spacy class Tokenizer ( modelkit . Model ): TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" , \"failing\" , \"test\" ], }, # fails as intended ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] You can run these test cases in the interactive programming tool of your choice (e.g. ipython , jupyter etc.) using the test method: Tokenizer () . test () # TEST 1: SUCCESS # TEST 2: SUCCESS # TEST 3: SUCCESS # TEST 4: SUCCESS # TEST 5: FAILED test failed on item # item = '<br/>a b c data<br/> e scientist' # expected = list instance # result = list instance Run using pytest It is also possible to automatically test all models using the pytest integration, using the Modelkit autotesting fixture . Woops, seems like we need to fix the last test!","title":"Tests"},{"location":"examples/nlp_sentiment/tokenizer/#input-and-output-specification","text":"It is good practice to specify inputs and outputs of models in production code This allows calls to be validated, thus ensuring consistency between calls, dependencies, services, and raising alerts when Models are not called as expected. This is also good for documentation, to understand how to use a given model, and during development to benefit from static type checking (e.g. with mypy ). modelkit allows you to define the expected input and output types of your model by subclassing Model[input_type, output_type] , where input_type and output_type can be standard Python types, dataclasses, or complex pydantic models. Let's add specification our Tokenizer to conclude this first part: from typing import List import modelkit import spacy class Tokenizer ( modelkit . Model [ str , List [ str ]]): TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" , \"failing\" , \"test\" ], }, # fails as intended ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] Calling the model with an unexpected type will raise a Modelkit ItemValidationException : Tokenizer () . predict ([ 1 , 2 , 3 , 4 ]) And mypy will raise errors if it encounters calls that are not correct: result : int = Tokenizer () . predict ( \"some text\" )","title":"Input and output specification"},{"location":"examples/nlp_sentiment/tokenizer/#conclusion","text":"That's it! In this modelkit introduction, you have learned: How to create a basic Model by inheriting from modelkit.Model and implementing a _predict method How to correctly load artefacts/assets by overriding the _load method How to leverage batch computing to speed up execution by implementing a _predict_batch method How to add tests to ensure everything works as intended using TEST_CASES right in your model definition How to add specification to your model's inputs and outputs using modelkit.Model[input_type, output_type]","title":"Conclusion"},{"location":"examples/nlp_sentiment/tokenizer/#final-tokenizer-code","text":"from typing import List import modelkit import spacy class Tokenizer ( modelkit . Model [ str , List [ str ]]): CONFIGURATIONS = { \"imdb_tokenizer\" : {}} TEST_CASES = [ { \"item\" : \"\" , \"result\" : []}, { \"item\" : \"NLP 101\" , \"result\" : [ \"nlp\" ]}, { \"item\" : \"I'm loving the spaCy 101 course !!!\u00f9*`^@\ud83d\ude00\" , \"result\" : [ \"loving\" , \"spacy\" , \"course\" ], }, { \"item\" : \"<br/>prepare things for IMDB<br/>\" , \"result\" : [ \"prepare\" , \"things\" , \"imdb\" ], }, { \"item\" : \"<br/>a b c data<br/> e scientist\" , \"result\" : [ \"data\" , \"scientist\" ], }, ] def _load ( self ): self . nlp = spacy . load ( \"en_core_web_sm\" , disable = [ \"parser\" , \"ner\" , \"tagger\" , \"lemmatizer\" , \"tok2vec\" , \"attribute_ruler\" , ], ) def _predict_batch ( self , texts ): texts = [ \" \" . join ( text . replace ( \"<br\" , \"\" ) . replace ( \"/>\" , \"\" ) . split ()) for text in texts ] return [ [ t . lower_ for t in text if t . is_ascii and len ( t ) > 1 and not ( t . is_punct or t . is_stop or t . is_digit ) ] for text in self . nlp . pipe ( texts , batch_size = len ( texts )) ] As you may have seen, there is a CONFIGURATIONS map in the class definition, we will cover it in the next section.","title":"Final Tokenizer code"},{"location":"examples/nlp_sentiment/vectorizer/","text":"In this section, we will fit and implement a custom text vectorizer based on the sentiment analysis IMDB dataset . It will be the opportunity to go through modelkit's assets management basics, learn how to fetch artifacts, and get deeper into its API. Installation \u00b6 First, let's download the IMDB reviews dataset: # download the remote archive curl https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz --output imdb.tar.gz # untar it tar -xvf imdb.tar.gz # remove the unsupervised directory we will not be using rm -rf aclImdb/train/unsup The different files consist in text reviews left by IMDB users, each file corresponding to one review. The dataset is organized with train and test directories, containing positive and negative examples in subfolders corresponding to the target classes. Creating an asset file \u00b6 Fitting \u00b6 First thing first, let's define a helper function to read through the training set. We will only be using generators to avoid loading the entire dataset in memory, (and later use Model.predict_gen to process them). import glob from typing import Generator def read_dataset ( path : str ) -> Generator [ str , None , None ]: for review in glob . glob ( os . path . join ( path , \"*.txt\" )): with open ( review , 'r' , encoding = 'utf-8' ) as f : yield f . read () We need to tokenize our dataset before fitting a Scikit-Learn TfidfVectorizer , Although sklearn includes a tokenizer, we will be using the one we implemented in the first section. import itertools import os training_set = itertools . chain ( read_dataset ( os . path . join ( \"aclImdb\" , \"train\" , \"pos\" )), read_dataset ( os . path . join ( \"aclImdb\" , \"train\" , \"neg\" )), ) tokenized_set = Tokenizer () . predict_gen ( training_set ) By using generators and the predict_gen method, we can to read huge texts corpora without filling up our memory. Each review will be tokenized one by one, but as we discussed we can also speed up execution by setting a batch_size greater than one, to process these reviews batch by batch. # here, an example with a 64 batch size tokenized_set = Tokenizer () . predict_gen ( training_set , batch_size = 64 ) We are all set! Let's fit our TfidfVectorizer using the tokenized_set , and disabling the embedded tokenizer. from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer ( tokenizer = lambda x : x , lowercase = False , max_df = 0.95 , min_df = 0.01 ) . fit ( tokenized_set ) We now need to save the vocabulary we just fitted to disk, before writing our on vectorizer Model . Of course, one could serialize the TfidfVectorizer , but there are many reasons why we would not want this in production code: TfidfVectorizer is only used to build the vocabulary using neat features such as min/max document frequencies, which are no longer useful during inference it requires a heavy code dependency: scikit-learn , only for vectorization pickling scikit-learn models come with some tricky issues relative to dependency, version and security It is also common practice to separate the research / training phase, from the production phase. As a result, we will be implementing our own vectorizer for production using modelkit , based on the vocabulary created with scikit-learn's TfidfVectorizer . We just need to write a list of strings to disk: # we only keep strings from the vocabulary # we will be using our own str -> int mapping vocabulary = next ( zip ( * sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]))) with open ( \"vocabulary.txt\" , \"w\" , encoding = \"utf-8\" ) as f : for row in vocabulary : f . write ( row + \" \\n \" ) Using the file in a Model \u00b6 In this subsection, you will learn how to leverage the basics of Modelkit's Assets management. Note Most of the features (assets remote storage, updates, versioning, etc.) will not be adressed in this tutorial, but are the way to go when things get serious. First, let's implement our Vectorizer as a modelkit.Model which loads the vocabulary.txt file we just created: import modelkit from typing import List class Vectorizer ( modelkit . Model ): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i Here, we define a CONFIGURATIONS class attribute, which lets modelkit know how to find the vocabulary. vocabulary.txt will be found in the current working directory, and its absolute path written to the Vectorizer.asset_path attribute before _load is called. Remote assets When assets are stored remotely, the remote versioned asset is retrieved and cached on the local disk before the _load method is called, and its local path is set in the self.asset_path attribute. modelkit guarantees that whenever _load is called, the file is present and its absolute path is written to Model.asset_path . What can be an asset An asset can be anything (a file, a directory), and the user is responsible for defining the loading logic in _load . You can refer to it with a relative path (which will be relative to the current working directeory), an absolute path, or a remote asset specification. In this case, it is rather straighforward: self.asset_path directly points to vocabulary.txt . In addition, the imdb_vectorizer configuration key can now be used to refer to the Vectorizer in a ModelLibrary object. This allows you to define multiple Vectorizer objects with different vocabularies, without rewriting the prediction logic. Prediction \u00b6 Now let us add a more complex prediction logic and input specification: import numpy as np import modelkit from typing import List class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i + 2 self . _vectorizer = np . vectorize ( lambda x : self . vocabulary . get ( x , 1 )) def _predict ( self , tokens , length = None , drop_oov = True ): vectorized = ( np . array ( self . _vectorizer ( tokens ), dtype = np . int ) if tokens else np . array ([], dtype = int ) ) if drop_oov and len ( vectorized ): vectorized = np . delete ( vectorized , vectorized == 1 ) if not length : return vectorized . tolist () result = np . zeros ( length ) vectorized = vectorized [: length ] result [: len ( vectorized )] = vectorized return result . tolist () We add several advanced features, first, to deal with out of vocabulary or padding tokens, we reserve the following integers: 0 for padding 1 for the unknown token (out-of-vocabulary words) 2+i for known vocabulary words We use np.vectorize to map a tokens list to an indices list, which we store in the _vectorize attribute in the _load . We also add keyword arguments to the _predict : length and drop_oov . These can be used during prediction as well, and would be passed to _predict or _predict_batch : vectorizer = Vectorizer () vectorizer . predict ( item , length = 10 , drop_oov = False ) vectorizer . predict_batch ( items , length = 10 , drop_oov = False ) vectorizer . predict_gen ( items , length = 10 , drop_oov = False ) Test cases \u00b6 Now let us add test cases too. The only trick here is that we have to add the information about our new keyword arguments when we want to test different values. To do so, we use the keyword_args field in the test cases: class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): ... TEST_CASES = [ { \"item\" : [], \"result\" : []}, { \"item\" : [], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 0 ] * 10 }, { \"item\" : [ \"movie\" ], \"result\" : [ 888 ]}, { \"item\" : [ \"unknown_token\" ], \"result\" : []}, { \"item\" : [ \"unknown_token\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 1 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"result\" : [ 888 , 1156 ]}, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 888 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 , \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, ] ... Final vectorizer \u00b6 Putting it all together, we obtain: import modelkit import numpy as np from typing import List class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} TEST_CASES = [ { \"item\" : [], \"result\" : []}, { \"item\" : [], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 0 ] * 10 }, { \"item\" : [ \"movie\" ], \"result\" : [ 888 ]}, { \"item\" : [ \"unknown_token\" ], \"result\" : []}, { \"item\" : [ \"unknown_token\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 1 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"result\" : [ 888 , 1156 ]}, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 888 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 , \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, ] def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i + 2 self . _vectorizer = np . vectorize ( lambda x : self . vocabulary . get ( x , 1 )) def _predict ( self , tokens , length = None , drop_oov = True ): vectorized = ( np . array ( self . _vectorizer ( tokens ), dtype = np . int ) if tokens else np . array ([], dtype = int ) ) if drop_oov and len ( vectorized ): vectorized = np . delete ( vectorized , vectorized == 1 ) if not length : return vectorized . tolist () result = np . zeros ( length ) vectorized = vectorized [: length ] result [: len ( vectorized )] = vectorized return result . tolist () In the next section, we will train a classifier using the Tokenizer and Vectorizer models we just created. This will show us how to compose and store models in modelkit .","title":"Vectorizer"},{"location":"examples/nlp_sentiment/vectorizer/#installation","text":"First, let's download the IMDB reviews dataset: # download the remote archive curl https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz --output imdb.tar.gz # untar it tar -xvf imdb.tar.gz # remove the unsupervised directory we will not be using rm -rf aclImdb/train/unsup The different files consist in text reviews left by IMDB users, each file corresponding to one review. The dataset is organized with train and test directories, containing positive and negative examples in subfolders corresponding to the target classes.","title":"Installation"},{"location":"examples/nlp_sentiment/vectorizer/#creating-an-asset-file","text":"","title":"Creating an asset file"},{"location":"examples/nlp_sentiment/vectorizer/#fitting","text":"First thing first, let's define a helper function to read through the training set. We will only be using generators to avoid loading the entire dataset in memory, (and later use Model.predict_gen to process them). import glob from typing import Generator def read_dataset ( path : str ) -> Generator [ str , None , None ]: for review in glob . glob ( os . path . join ( path , \"*.txt\" )): with open ( review , 'r' , encoding = 'utf-8' ) as f : yield f . read () We need to tokenize our dataset before fitting a Scikit-Learn TfidfVectorizer , Although sklearn includes a tokenizer, we will be using the one we implemented in the first section. import itertools import os training_set = itertools . chain ( read_dataset ( os . path . join ( \"aclImdb\" , \"train\" , \"pos\" )), read_dataset ( os . path . join ( \"aclImdb\" , \"train\" , \"neg\" )), ) tokenized_set = Tokenizer () . predict_gen ( training_set ) By using generators and the predict_gen method, we can to read huge texts corpora without filling up our memory. Each review will be tokenized one by one, but as we discussed we can also speed up execution by setting a batch_size greater than one, to process these reviews batch by batch. # here, an example with a 64 batch size tokenized_set = Tokenizer () . predict_gen ( training_set , batch_size = 64 ) We are all set! Let's fit our TfidfVectorizer using the tokenized_set , and disabling the embedded tokenizer. from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer ( tokenizer = lambda x : x , lowercase = False , max_df = 0.95 , min_df = 0.01 ) . fit ( tokenized_set ) We now need to save the vocabulary we just fitted to disk, before writing our on vectorizer Model . Of course, one could serialize the TfidfVectorizer , but there are many reasons why we would not want this in production code: TfidfVectorizer is only used to build the vocabulary using neat features such as min/max document frequencies, which are no longer useful during inference it requires a heavy code dependency: scikit-learn , only for vectorization pickling scikit-learn models come with some tricky issues relative to dependency, version and security It is also common practice to separate the research / training phase, from the production phase. As a result, we will be implementing our own vectorizer for production using modelkit , based on the vocabulary created with scikit-learn's TfidfVectorizer . We just need to write a list of strings to disk: # we only keep strings from the vocabulary # we will be using our own str -> int mapping vocabulary = next ( zip ( * sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]))) with open ( \"vocabulary.txt\" , \"w\" , encoding = \"utf-8\" ) as f : for row in vocabulary : f . write ( row + \" \\n \" )","title":"Fitting"},{"location":"examples/nlp_sentiment/vectorizer/#using-the-file-in-a-model","text":"In this subsection, you will learn how to leverage the basics of Modelkit's Assets management. Note Most of the features (assets remote storage, updates, versioning, etc.) will not be adressed in this tutorial, but are the way to go when things get serious. First, let's implement our Vectorizer as a modelkit.Model which loads the vocabulary.txt file we just created: import modelkit from typing import List class Vectorizer ( modelkit . Model ): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i Here, we define a CONFIGURATIONS class attribute, which lets modelkit know how to find the vocabulary. vocabulary.txt will be found in the current working directory, and its absolute path written to the Vectorizer.asset_path attribute before _load is called. Remote assets When assets are stored remotely, the remote versioned asset is retrieved and cached on the local disk before the _load method is called, and its local path is set in the self.asset_path attribute. modelkit guarantees that whenever _load is called, the file is present and its absolute path is written to Model.asset_path . What can be an asset An asset can be anything (a file, a directory), and the user is responsible for defining the loading logic in _load . You can refer to it with a relative path (which will be relative to the current working directeory), an absolute path, or a remote asset specification. In this case, it is rather straighforward: self.asset_path directly points to vocabulary.txt . In addition, the imdb_vectorizer configuration key can now be used to refer to the Vectorizer in a ModelLibrary object. This allows you to define multiple Vectorizer objects with different vocabularies, without rewriting the prediction logic.","title":"Using the file in a Model"},{"location":"examples/nlp_sentiment/vectorizer/#prediction","text":"Now let us add a more complex prediction logic and input specification: import numpy as np import modelkit from typing import List class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i + 2 self . _vectorizer = np . vectorize ( lambda x : self . vocabulary . get ( x , 1 )) def _predict ( self , tokens , length = None , drop_oov = True ): vectorized = ( np . array ( self . _vectorizer ( tokens ), dtype = np . int ) if tokens else np . array ([], dtype = int ) ) if drop_oov and len ( vectorized ): vectorized = np . delete ( vectorized , vectorized == 1 ) if not length : return vectorized . tolist () result = np . zeros ( length ) vectorized = vectorized [: length ] result [: len ( vectorized )] = vectorized return result . tolist () We add several advanced features, first, to deal with out of vocabulary or padding tokens, we reserve the following integers: 0 for padding 1 for the unknown token (out-of-vocabulary words) 2+i for known vocabulary words We use np.vectorize to map a tokens list to an indices list, which we store in the _vectorize attribute in the _load . We also add keyword arguments to the _predict : length and drop_oov . These can be used during prediction as well, and would be passed to _predict or _predict_batch : vectorizer = Vectorizer () vectorizer . predict ( item , length = 10 , drop_oov = False ) vectorizer . predict_batch ( items , length = 10 , drop_oov = False ) vectorizer . predict_gen ( items , length = 10 , drop_oov = False )","title":"Prediction"},{"location":"examples/nlp_sentiment/vectorizer/#test-cases","text":"Now let us add test cases too. The only trick here is that we have to add the information about our new keyword arguments when we want to test different values. To do so, we use the keyword_args field in the test cases: class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): ... TEST_CASES = [ { \"item\" : [], \"result\" : []}, { \"item\" : [], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 0 ] * 10 }, { \"item\" : [ \"movie\" ], \"result\" : [ 888 ]}, { \"item\" : [ \"unknown_token\" ], \"result\" : []}, { \"item\" : [ \"unknown_token\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 1 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"result\" : [ 888 , 1156 ]}, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 888 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 , \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, ] ...","title":"Test cases"},{"location":"examples/nlp_sentiment/vectorizer/#final-vectorizer","text":"Putting it all together, we obtain: import modelkit import numpy as np from typing import List class Vectorizer ( modelkit . Model [ List [ str ], List [ int ]]): CONFIGURATIONS = { \"imdb_vectorizer\" : { \"asset\" : \"vocabulary.txt\" }} TEST_CASES = [ { \"item\" : [], \"result\" : []}, { \"item\" : [], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 0 ] * 10 }, { \"item\" : [ \"movie\" ], \"result\" : [ 888 ]}, { \"item\" : [ \"unknown_token\" ], \"result\" : []}, { \"item\" : [ \"unknown_token\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 1 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"result\" : [ 888 , 1156 ]}, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 }, \"result\" : [ 888 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, { \"item\" : [ \"movie\" , \"unknown_token\" , \"scenes\" ], \"keyword_args\" : { \"length\" : 10 , \"drop_oov\" : False }, \"result\" : [ 888 , 1 , 1156 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ], }, ] def _load ( self ): self . vocabulary = {} with open ( self . asset_path , \"r\" , encoding = \"utf-8\" ) as f : for i , k in enumerate ( f ): self . vocabulary [ k . strip ()] = i + 2 self . _vectorizer = np . vectorize ( lambda x : self . vocabulary . get ( x , 1 )) def _predict ( self , tokens , length = None , drop_oov = True ): vectorized = ( np . array ( self . _vectorizer ( tokens ), dtype = np . int ) if tokens else np . array ([], dtype = int ) ) if drop_oov and len ( vectorized ): vectorized = np . delete ( vectorized , vectorized == 1 ) if not length : return vectorized . tolist () result = np . zeros ( length ) vectorized = vectorized [: length ] result [: len ( vectorized )] = vectorized return result . tolist () In the next section, we will train a classifier using the Tokenizer and Vectorizer models we just created. This will show us how to compose and store models in modelkit .","title":"Final vectorizer"},{"location":"library/caching/","text":"Prediction caching \u00b6 It is possible to use a redis caching mechanism to cache all calls to predict for a ModelLibrary using the enable_redis_cache , cache_host , and cache_port settings of the LibrarySettings . This has to be enabled for each model by setting the cache_predictions model setting to True . The caching works on individual items, before making a prediction with the methods in the Model class, it will attempt to see if an available prediction is already available in the cache. Predictions in the cache are keyed by a hash of the passed item alongside the key of the model (the key used in the configuration of the model). When a prediction on a batch of items is requested, the Model will sieve through each item and attempt to find cached predictions for each. It will therefore only recompute predictions for the select items that do not appear in the cache.","title":"Caching"},{"location":"library/caching/#prediction-caching","text":"It is possible to use a redis caching mechanism to cache all calls to predict for a ModelLibrary using the enable_redis_cache , cache_host , and cache_port settings of the LibrarySettings . This has to be enabled for each model by setting the cache_predictions model setting to True . The caching works on individual items, before making a prediction with the methods in the Model class, it will attempt to see if an available prediction is already available in the cache. Predictions in the cache are keyed by a hash of the passed item alongside the key of the model (the key used in the configuration of the model). When a prediction on a batch of items is requested, the Model will sieve through each item and attempt to find cached predictions for each. It will therefore only recompute predictions for the select items that do not appear in the cache.","title":"Prediction caching"},{"location":"library/lazy_loading/","text":"Usually, all model assets are loaded as soon as the ModelLibrary is instantiated. Sometimes this is not desirable, notably when using PySpark. Thus, when lazy_loading=True the ModelLibrary tries to delay the loading and deserialization of the assets as much as possible. You can also set this behavior by setting MODELKIT_LAZY_LOADING=True in your environment. Specifically: When the ModelLibrary is instantiated nothing really happens: the Model object is instantiated without deserializing the asset. When ModelLibrary.get is called the first time, the Model 's asset is downloaded (via ModelLibrary._load ) to a local directory and deserialized. It is also possible to explicitly ask the ModelLibrary to load all required_models at once by calling ModelLibrary.preload .","title":"Lazy loading"},{"location":"library/model_library/","text":"The ModelLibrary is the primary object that provides predictions from models. ModelLibrary objects can have a number of settings, passed as a dictionary upon initialization ModelLibrary(required_models = ..., settings = ...) . These parameters are exploited by the ModelLibrary directly and set as the service_settings attribute of Model objects. Main arguments: models a module, Model (or list of either) which is used to find configurations of the models to be loaded configuration allows you to provide an explicit configuration to override the ones present in the Model.CONFIGURATIONS attributes. required_models a list of models to load by the library. This allows you to restrict the models that will actually be loaded into memory. By default all models from models are loaded ( required_models=None ), pass the empty list to not load any models (or use the lazy mode). Names in this list have to be defined in the configurations of the models passed via models . You can pass a dictionary to override the asset for each model. Additionally, the ModelLibrary takes a settings keyword argument which allows you to provide advanced settings: Model instance to be created for the required models. This is useful to download the assets for example with TF serving lazy_loading : when True, this will cause the assets to be loaded lazily. This is useful for pyspark jobs with model object that are not serializable enable_tf_serving , tf_serving_port , tf_serving_host : Set parameters related to the serving of TF models (see here ). assetsmanager_settings : Parameters passed to the assets.manager.AssetsManager override_assets_dir : Specify an alternative assets directory from which the prediction service will try to use assets before falling back to the normal assets directory. It is used to test new assets without having to push them in the main storage.","title":"Model library"},{"location":"library/overview/","text":"The main concepts in modelkit are Model and the ModelLibrary . The ModelLibrary instantiates and configures Model objects and keeps track of them during execution. Model objects can then be requested via ModelLibrary.get , and used to make predictions via Model.predict . The ML logic is written in each Model 's predict functions, typically inside a module. Model and ModelLibrary \u00b6 The normal way to use modelkit models is by creating models by subclassing the modelkit.Model class, and adding a configuration, then creating a ModelLibrary by instantiating a ModelLibrary with a set of models. from modelkit import ModelLibrary , Model # Create a Model subclass class MyModel ( Model ): # Give it a name CONFIGURATIONS = { \"my_favorite_model\" : {}} # Write some prediction logic def _predict ( self , item ): return item # Create the model library library = ModelLibrary ( models = MyModel ) # Get the model model = library . get ( \"my_favorite_model\" ) # Get predictions model ( \"hello world\" ) # returns hello world In the tutorial you will learn that: Models can have an asset linked to them, to store parameters, weights, or anything really. This asset is loaded and deserialized when the ModelLibrary instantiates the object. Models can depend on other models and share objects in memory (in particular, they can share assets). Only the minimal subset of models is loaded when a given model is required. Model inputs and outputs can be systematically validated using pydantic Models can implement vectorized logic to make faster predictions. Models can implement asynchronous logic and be called either way. Models can serve Tensorflow models conveniently","title":"Overview"},{"location":"library/overview/#model-and-modellibrary","text":"The normal way to use modelkit models is by creating models by subclassing the modelkit.Model class, and adding a configuration, then creating a ModelLibrary by instantiating a ModelLibrary with a set of models. from modelkit import ModelLibrary , Model # Create a Model subclass class MyModel ( Model ): # Give it a name CONFIGURATIONS = { \"my_favorite_model\" : {}} # Write some prediction logic def _predict ( self , item ): return item # Create the model library library = ModelLibrary ( models = MyModel ) # Get the model model = library . get ( \"my_favorite_model\" ) # Get predictions model ( \"hello world\" ) # returns hello world In the tutorial you will learn that: Models can have an asset linked to them, to store parameters, weights, or anything really. This asset is loaded and deserialized when the ModelLibrary instantiates the object. Models can depend on other models and share objects in memory (in particular, they can share assets). Only the minimal subset of models is loaded when a given model is required. Model inputs and outputs can be systematically validated using pydantic Models can implement vectorized logic to make faster predictions. Models can implement asynchronous logic and be called either way. Models can serve Tensorflow models conveniently","title":"Model and ModelLibrary"},{"location":"library/models/asynchronous_models/","text":"Although we have only describe synchronous models so far, modelkit allows you to write asynchronous code in Model objects and use the exact same functionality as described. To do so, simply subclass the modelkit.AsyncModel instead of Model . class SomeAsyncModel ( AsyncModel ): CONFIGURATIONS = { \"async_model\" : {}} async def _predict ( self , item , ** kwargs ): await asyncio . sleep ( 0 ) return item Now, it is required to write the _predict or _predict_batch methods with async def , and you can use await expressions. Similarly, the predict and predict_batch methods become async, and predict_gen is an async generator: m = SomeAsyncModel () await m . predict ( ... ) await m . predict_batch ( ... ) async for res in m . predict_gen ( ... ): ... Async and sync composition \u00b6 To make it easy to have both synchronous and asynchronous models in the same prediction call stack, modelkit atttempts to detect which context it is in and tries to make asynchronous models available even in synchronous contexts. Sync in async \u00b6 If you have an asynchronous Model that depends on a synchronous model, there is nothing to do, you can simply call it as usual model_a (async) -depends-on-> model_b In model_b._predict this causes no issues async def _predict ( self , item ): ... something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ... Async in sync \u00b6 The opposite situation wherein you call an asynchronous model in a synchronous context is more annoying: model_a -depends-on-> model_b (async) Indeed, this code would be invalid since predict returns a coroutine def _predict ( self , item ): ... something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ... To work around this, when modelkit encounters an AsyncModel in a synchronous context, it will wrap it in a WrappedAsyncModel that exposes \"syncified\" versions of the predict functions using asgiref . As a result, the model_a will have a different object in its dependency, making the following valid. def _predict ( self , item ): ... assert isinstance ( self . model_dependencies [ \"model_b\" ], WrappedAsyncModel ) something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ... TL;DR, if you want to use asynchronous logic in the modelkit code, make sure that your dependencies do not have a sync-with-async-dependency in the chain, otherwise this may create issues.","title":"Async"},{"location":"library/models/asynchronous_models/#async-and-sync-composition","text":"To make it easy to have both synchronous and asynchronous models in the same prediction call stack, modelkit atttempts to detect which context it is in and tries to make asynchronous models available even in synchronous contexts.","title":"Async and sync composition"},{"location":"library/models/asynchronous_models/#sync-in-async","text":"If you have an asynchronous Model that depends on a synchronous model, there is nothing to do, you can simply call it as usual model_a (async) -depends-on-> model_b In model_b._predict this causes no issues async def _predict ( self , item ): ... something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ...","title":"Sync in async"},{"location":"library/models/asynchronous_models/#async-in-sync","text":"The opposite situation wherein you call an asynchronous model in a synchronous context is more annoying: model_a -depends-on-> model_b (async) Indeed, this code would be invalid since predict returns a coroutine def _predict ( self , item ): ... something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ... To work around this, when modelkit encounters an AsyncModel in a synchronous context, it will wrap it in a WrappedAsyncModel that exposes \"syncified\" versions of the predict functions using asgiref . As a result, the model_a will have a different object in its dependency, making the following valid. def _predict ( self , item ): ... assert isinstance ( self . model_dependencies [ \"model_b\" ], WrappedAsyncModel ) something = self . model_dependencies [ \"model_b\" ] . predict ( ... ) ... return ... TL;DR, if you want to use asynchronous logic in the modelkit code, make sure that your dependencies do not have a sync-with-async-dependency in the chain, otherwise this may create issues.","title":"Async in sync"},{"location":"library/models/batching/","text":"Oftentimes, when using external libraries, it is much faster to compute predictions with batches of items. This is true with deep learning, but also when writing code with libraries like numpy . For this reason, it is possible to define Model with batched prediction code, by overriding _predict_batch instead of _predict : class MyModel ( Model ): def _predict ( self , item ): return item # OR def _predict_batch ( self , items ): return items Whichever one you decide to implement, modelkit will still expose the same methods predict , predict_batch and predict_gen . Typically, one would first implement _predict to get the logic right, and later, if needed, implement _predict_batch to improve performance. Warning Don't override both _predict and _predict_batch . This will raise an error Example In this example, we implement a dummy Model that computes the position of the min in a list using np.argmin . In one version the code is not vectorized (it operates on a single item) and in the other one it is (a whole batched is processed at once). The vectorized version is ~50% faster import random import timeit from modelkit.core.model import Model import numpy as np # Create some data data = [] base_item = list ( range ( 100 )) for _ in range ( 128 ): random . shuffle ( base_item ) data . append ( list ( base_item )) # This model is not vectorized, `np.argmin` # will be called individually for each batch class MinModel ( Model ): def _predict ( self , item ): return np . argmin ( item ) m = MinModel () # This model is vectorized, `np.argmin` # is called over a whole batch class MinBatchedModel ( Model ): def _predict_batch ( self , items ): return np . argmin ( items , axis = 1 ) m_batched = MinBatchedModel () # They do return the same results assert m . predict_batch ( data ) == m_batched . predict_batch ( data ) # The batched model is ~50% slower timeit . timeit ( lambda : m . predict_batch ( data ), number = 1000 ) # The batched model is ~50% slower timeit . timeit ( lambda : m_batched . predict_batch ( data ), number = 1000 ) # Even more so with a larger batch size timeit . timeit ( lambda : m_batched . predict_batch ( data , batch_size = 128 ), number = 1000 ) Controling batch size \u00b6 The default batch size for the Model object is controlled by its batch_size attribute. It defaults to None , which means that _predict_batch will by default always get: a single, full length batch with all the items when called via predict_batch as many batches of size one as there are items when called via predict_gen It is possible to control the batch size for each call to _predict_batch , by using: items = [ 1 , 2 , 3 , 4 ] predictions = model . predict_batch ( items , batch_size = 2 ) for p in model . predict_gen ( iter ( items ), batch_size = 2 ): ... # predictions will be computed in batches of two This is useful to avoid computing batches that are too large and may take up too much memory. Note that, although modelkit will attempt to build batches of even size, this is not always the case: remaining items if you request 10 predictions with a batch size of 3, the last batch will only contain one. caching when caching, modelkit will yield if sufficiently many predictions can be fetched in the cache, and compute the rest, wich will lead to smaller batches than expected. If you do need to access the number of items in a batch, use len(items) inside the _predict_batch . If you need to make sure that it is contant, you will have to implement padding yourself. Batches When using predict_gen with a model with _predict_batch implemented, modelkit will construct batches, while still yielding items one by one.","title":"Batching"},{"location":"library/models/batching/#controling-batch-size","text":"The default batch size for the Model object is controlled by its batch_size attribute. It defaults to None , which means that _predict_batch will by default always get: a single, full length batch with all the items when called via predict_batch as many batches of size one as there are items when called via predict_gen It is possible to control the batch size for each call to _predict_batch , by using: items = [ 1 , 2 , 3 , 4 ] predictions = model . predict_batch ( items , batch_size = 2 ) for p in model . predict_gen ( iter ( items ), batch_size = 2 ): ... # predictions will be computed in batches of two This is useful to avoid computing batches that are too large and may take up too much memory. Note that, although modelkit will attempt to build batches of even size, this is not always the case: remaining items if you request 10 predictions with a batch size of 3, the last batch will only contain one. caching when caching, modelkit will yield if sufficiently many predictions can be fetched in the cache, and compute the rest, wich will lead to smaller batches than expected. If you do need to access the number of items in a batch, use len(items) inside the _predict_batch . If you need to make sure that it is contant, you will have to implement padding yourself. Batches When using predict_gen with a model with _predict_batch implemented, modelkit will construct batches, while still yielding items one by one.","title":"Controling batch size"},{"location":"library/models/configuring_models/","text":"As models become more complicated they are attached to different assets or other models. We will need to instanciate them through the ModelLibrary object which will take care of all this for us. To do so, we have to configure our model: give it a name, and possibly assets, dependencies, adding test cases, etc. Models are made available to clients using modelkit by specifying them using the CONFIGURATIONS class attribute: class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : {} } def _predict ( self , item ): return \"something\" Right now, we have only given it a name \"simple\" which makes the model available to other models via the ModelLibrary . The rest of the configuration is empty but we will add to it at the next section. Assuming that SimpleModel is defined in my_module.my_models , it is now accessible via: from modelkit.core import ModelLibrary import my_module.my_models p = ModelLibrary ( models = my_module . my_models ) m = p . get ( \"simple\" ) See Organization for more information on how to organize your models. Model settings \u00b6 The simplest configuration options are model_settings : from modelkit.core.model import Model class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : { \"model_settings\" : { \"value\" : \"something\" }}, \"simple2\" : { \"model_settings\" : { \"value\" : \"something2\" }} } def _predict ( self , item ): return self . model_settings [ \"value\" ] Now, there are two versions of the model available, simple and simple2 : from modelkit.core import ModelLibrary p = ModelLibrary ( models = SimpleModel ) m = p . get ( \"simple\" ) print ( m ({})) m2 = p . get ( \"simple2\" ) print ( m2 ({})) It will print \"something\" and \"something2\" . Model attributes \u00b6 In general, Model have several attributes set by the ModelLibrary when they are loaded: asset_path the path to the asset set in the Model 's configuration configuration_key the key of the model's configuration model_dependencies a dictionary of Model dependencies model_settings the model_settings as passed at initialization service_settings the settings of the ModelLibrary that created the model batch_size the batch size for the model: if _predict_batch is implemented it will default to getting batches of this size. It defaults to None , which means \"no batching\" Asset class \u00b6 It is sometimes useful for a given asset in memory to serve many different Model objects. It is possibly by using the model_dependencies to point to a parent Model that is the only one to load the asset via _load . In this case, we may not want the parent asset-bearing Model object to implement predict at all. This is what an modelkit.core.model.Asset is. Note In fact, it is defined the other way around: Model s are Asset s with a predict function, and thus Model inherits from Asset . Note There are two ways to use a data asset in a Model : either load it directly via its configuration and the _load , or package it in an Asset and use the deserialized object via model dependencies.","title":"Configuring models"},{"location":"library/models/configuring_models/#model-settings","text":"The simplest configuration options are model_settings : from modelkit.core.model import Model class SimpleModel ( Model ): CONFIGURATIONS = { \"simple\" : { \"model_settings\" : { \"value\" : \"something\" }}, \"simple2\" : { \"model_settings\" : { \"value\" : \"something2\" }} } def _predict ( self , item ): return self . model_settings [ \"value\" ] Now, there are two versions of the model available, simple and simple2 : from modelkit.core import ModelLibrary p = ModelLibrary ( models = SimpleModel ) m = p . get ( \"simple\" ) print ( m ({})) m2 = p . get ( \"simple2\" ) print ( m2 ({})) It will print \"something\" and \"something2\" .","title":"Model settings"},{"location":"library/models/configuring_models/#model-attributes","text":"In general, Model have several attributes set by the ModelLibrary when they are loaded: asset_path the path to the asset set in the Model 's configuration configuration_key the key of the model's configuration model_dependencies a dictionary of Model dependencies model_settings the model_settings as passed at initialization service_settings the settings of the ModelLibrary that created the model batch_size the batch size for the model: if _predict_batch is implemented it will default to getting batches of this size. It defaults to None , which means \"no batching\"","title":"Model attributes"},{"location":"library/models/configuring_models/#asset-class","text":"It is sometimes useful for a given asset in memory to serve many different Model objects. It is possibly by using the model_dependencies to point to a parent Model that is the only one to load the asset via _load . In this case, we may not want the parent asset-bearing Model object to implement predict at all. This is what an modelkit.core.model.Asset is. Note In fact, it is defined the other way around: Model s are Asset s with a predict function, and thus Model inherits from Asset . Note There are two ways to use a data asset in a Model : either load it directly via its configuration and the _load , or package it in an Asset and use the deserialized object via model dependencies.","title":"Asset class"},{"location":"library/models/model_with_load/","text":"A model can implement a _load method that is called by modelkit either when loading the model through a ModelLibrary or when instantiating it. This allows you to load information from a files or folders stored locally (or retrieved from an object store). These can contain arbitrary files, folders, parameters, optimized data structures, or anything really. modelkit ensures that it is only ever called once, regardless of how many times you want to use your model. modelkit refers to these supporting files and directories used to load model as assets . Defining a model asset \u00b6 The model asset is specified in the CONFIGURATIONS under the asset key: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"some_file.txt\" } } def _load ( self ): with open ( self . asset_path ) as f : ... When the _load method is called, the object will have an asset_path attribute that points to the absolute path of the asset locally. modelkit will ensure that the file is actually present before _load is called. The _load method is used to load the relevant information from the asset file(s). Asset convention and resolution \u00b6 The string definition of an asset in the CONFIGURATIONS can refer to different things: an absolute path to a local file a relative path to a local file (relative to the current working directory) a relative path to a local file (relative to the \"assets directory\" a directory typically set with an environment variable MODELKIT_ASSETS_DIR ) an asset specification which can refer to assets stored remotely We encourage you to use Asset specification with remote asset storage , in order to get all the power of modelkit asset system. Other dependencies usable during _load \u00b6 Whenever _load is called, modelkit ensures that all other dependencies are already loaded and ready to go. These include: asset_path the path to the asset (which is guaranteed to be present when it is called) model_settings as present in the CONFIGURATIONS model_dependencies are fully loaded service_settings the settings of the ModelLibrary _load vs. __init__ \u00b6 Assets retrieval, dependencies management, are all handled for you by modelkit.ModelLibrary . For some features, it is necessary to instantiate the objects first, and only after resolve all assets and dependencies (e.g. in lazy mode, which is useful for Spark for example). As a result it is only guaranteed that attributes will be present when _load is called, rather than __init__ . This is why it is not in general a good idea to override __init__ (or instantiate the models without the help of a ModelLibrary .","title":"Model loading"},{"location":"library/models/model_with_load/#defining-a-model-asset","text":"The model asset is specified in the CONFIGURATIONS under the asset key: class ModelWithAsset ( Model ): CONFIGURATIONS = { \"model_with_asset\" : { \"asset\" : \"some_file.txt\" } } def _load ( self ): with open ( self . asset_path ) as f : ... When the _load method is called, the object will have an asset_path attribute that points to the absolute path of the asset locally. modelkit will ensure that the file is actually present before _load is called. The _load method is used to load the relevant information from the asset file(s).","title":"Defining a model asset"},{"location":"library/models/model_with_load/#asset-convention-and-resolution","text":"The string definition of an asset in the CONFIGURATIONS can refer to different things: an absolute path to a local file a relative path to a local file (relative to the current working directory) a relative path to a local file (relative to the \"assets directory\" a directory typically set with an environment variable MODELKIT_ASSETS_DIR ) an asset specification which can refer to assets stored remotely We encourage you to use Asset specification with remote asset storage , in order to get all the power of modelkit asset system.","title":"Asset convention and resolution"},{"location":"library/models/model_with_load/#other-dependencies-usable-during-_load","text":"Whenever _load is called, modelkit ensures that all other dependencies are already loaded and ready to go. These include: asset_path the path to the asset (which is guaranteed to be present when it is called) model_settings as present in the CONFIGURATIONS model_dependencies are fully loaded service_settings the settings of the ModelLibrary","title":"Other dependencies usable during _load"},{"location":"library/models/model_with_load/#_load-vs-__init__","text":"Assets retrieval, dependencies management, are all handled for you by modelkit.ModelLibrary . For some features, it is necessary to instantiate the objects first, and only after resolve all assets and dependencies (e.g. in lazy mode, which is useful for Spark for example). As a result it is only guaranteed that attributes will be present when _load is called, rather than __init__ . This is why it is not in general a good idea to override __init__ (or instantiate the models without the help of a ModelLibrary .","title":"_load vs. __init__"},{"location":"library/models/models_with_dependencies/","text":"modelkit models are composable : a Model can depend on other Model s, and exploit their attributes and predictions. The ModelLibrary ensures that whenever _load or the _predict_* function are called, these models are loaded and present in the model_dependencies dictionary: For example your can set your model's configuration to have access to two other Model objects: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"sentence_piece_cleaner\" , \"sentence_piece_vectorizer\" } } } def _predict ( self , item ): # The `model_dependencies` attribute contains fully loaded dependent # models which can be used directly: cleaned = self . models_dependencies [ \"sentence_piece_cleaner\" ] . predict ( item [ \"text\" ]) ... Renaming dependencies \u00b6 In addition, it is possible to rename dependencies on the fly by providing a mapping to model_dependencies . This is useful in order to keep the same predict code, even though dependencies have changed: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner\" , } }, \"some_model_2\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner_2\" } } } def _predict ( self , item ): # Will call `sentence_piece_cleaner` in the `some_model` model and # `sentence_piece_cleaner_2` in the `some_model_2` model return self . model_dependencies [ \"cleaner\" ] . predict ( item ) Renaming several dependencies All your dependencies must be mapped (with an identity mapping for not renamed ones) e.g. : CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"renamed_model_1\" : \"model_1\" , \"model_2\" : \"model_2\" , } } } Dependencies in load \u00b6 Whenever a model's _load method is called, modelkit guarantees that all dependent models are also loaded, such that the model_dependencies attribute is populated by completely loaded models too. It is therefore possible to use the model_dependencies in the _load method too: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"sentence_piece_cleaner\" , \"sentence_piece_vectorizer\" } } } def _load ( self , item ): # The `model_dependencies` attribute contains fully loaded dependent # models which can be used directly: ...","title":"Dependencies"},{"location":"library/models/models_with_dependencies/#renaming-dependencies","text":"In addition, it is possible to rename dependencies on the fly by providing a mapping to model_dependencies . This is useful in order to keep the same predict code, even though dependencies have changed: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner\" , } }, \"some_model_2\" : { \"model_dependencies\" : { \"cleaner\" : \"sentence_piece_cleaner_2\" } } } def _predict ( self , item ): # Will call `sentence_piece_cleaner` in the `some_model` model and # `sentence_piece_cleaner_2` in the `some_model_2` model return self . model_dependencies [ \"cleaner\" ] . predict ( item ) Renaming several dependencies All your dependencies must be mapped (with an identity mapping for not renamed ones) e.g. : CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"renamed_model_1\" : \"model_1\" , \"model_2\" : \"model_2\" , } } }","title":"Renaming dependencies"},{"location":"library/models/models_with_dependencies/#dependencies-in-load","text":"Whenever a model's _load method is called, modelkit guarantees that all dependent models are also loaded, such that the model_dependencies attribute is populated by completely loaded models too. It is therefore possible to use the model_dependencies in the _load method too: class SomeModel ( Model ): CONFIGURATIONS = { \"some_model\" : { \"model_dependencies\" : { \"sentence_piece_cleaner\" , \"sentence_piece_vectorizer\" } } } def _load ( self , item ): # The `model_dependencies` attribute contains fully loaded dependent # models which can be used directly: ...","title":"Dependencies in load"},{"location":"library/models/organizing/","text":"Models organization \u00b6 modelkit encourages you to organise models in python packages which can be tested and shared between members of the same team. ModelLibrary from a package \u00b6 For example, assuming we have a modelkit model configured as my_favorite_model somewhere under the my_models module. from modelkit import ModelLibrary import my_models # contains subclasses of `Model` # Create the library # This downloads assets and instantiates model_dependencies library = ModelLibrary ( models = my_models ) model = library . get ( \"my_favorite_model\" ) Shortcuts For development, it is also possible to load a single model without a ModelLibrary : from modelkit import load_model model = load_model ( \"my_favorite_model\" , models = \"my_models\" ) If you have set the MODELKIT_DEFAULT_PACKAGE environment variable, you can also skip the models=... part. Organizing code \u00b6 A typical modelkit model repository follows the same organisation as any other Python package. PYTHONPATH \u2514\u2500\u2500 my_ml_package | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 module.py | \u251c\u2500\u2500 module_2.py # defines sub_model | \u251c\u2500\u2500 subpackage | \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 sub_module.py | \u2502 | ... | \u2502 ... modelkit can make all packages available in a single ModelLibrary as so: from modelkit import ModelLibrary import my_ml_package service = ModelLibrary ( models = my_ml_package ) Note: It is also possible to refer to a sub module ModelLibrary(models=package.subpackage) the Model classes themselves ModelLibrary(models=SomeModelClass) , string package names ModelLibrary(models=\"package.module_2\") or any combination of the above ModelLibrary(models=[package.subpackage, SomeModelClass]) In order to restrict the models that are actually being loaded, pass a list of required_models keys to the ModelLibrary instantiation: service = ModelLibrary ( models = [ package . module_2 , package . subpackage ], required_models = [ \"some_model\" ] ) Abstract models \u00b6 It is possible to define models that inherits from an abstract model in order to share common behavior. It only requires to not set CONFIGURATIONS dict for those models to be ignored from the configuration steps. For instance, it can be usefull to implement common prediction algorithm on different data assets. class BaseModel ( Model ): def _predict ( self , item , ** kwargs ): ... class DerivedModel ( BaseModel ): CONFIGURATIONS = { \"derived\" : { \"asset\" : \"something.txt\" }} def _load ( self ): ...","title":"Organizing models"},{"location":"library/models/organizing/#models-organization","text":"modelkit encourages you to organise models in python packages which can be tested and shared between members of the same team.","title":"Models organization"},{"location":"library/models/organizing/#modellibrary-from-a-package","text":"For example, assuming we have a modelkit model configured as my_favorite_model somewhere under the my_models module. from modelkit import ModelLibrary import my_models # contains subclasses of `Model` # Create the library # This downloads assets and instantiates model_dependencies library = ModelLibrary ( models = my_models ) model = library . get ( \"my_favorite_model\" ) Shortcuts For development, it is also possible to load a single model without a ModelLibrary : from modelkit import load_model model = load_model ( \"my_favorite_model\" , models = \"my_models\" ) If you have set the MODELKIT_DEFAULT_PACKAGE environment variable, you can also skip the models=... part.","title":"ModelLibrary from a package"},{"location":"library/models/organizing/#organizing-code","text":"A typical modelkit model repository follows the same organisation as any other Python package. PYTHONPATH \u2514\u2500\u2500 my_ml_package | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 module.py | \u251c\u2500\u2500 module_2.py # defines sub_model | \u251c\u2500\u2500 subpackage | \u2502 \u251c\u2500\u2500 __init__.py | \u2502 \u251c\u2500\u2500 sub_module.py | \u2502 | ... | \u2502 ... modelkit can make all packages available in a single ModelLibrary as so: from modelkit import ModelLibrary import my_ml_package service = ModelLibrary ( models = my_ml_package ) Note: It is also possible to refer to a sub module ModelLibrary(models=package.subpackage) the Model classes themselves ModelLibrary(models=SomeModelClass) , string package names ModelLibrary(models=\"package.module_2\") or any combination of the above ModelLibrary(models=[package.subpackage, SomeModelClass]) In order to restrict the models that are actually being loaded, pass a list of required_models keys to the ModelLibrary instantiation: service = ModelLibrary ( models = [ package . module_2 , package . subpackage ], required_models = [ \"some_model\" ] )","title":"Organizing code"},{"location":"library/models/organizing/#abstract-models","text":"It is possible to define models that inherits from an abstract model in order to share common behavior. It only requires to not set CONFIGURATIONS dict for those models to be ignored from the configuration steps. For instance, it can be usefull to implement common prediction algorithm on different data assets. class BaseModel ( Model ): def _predict ( self , item , ** kwargs ): ... class DerivedModel ( BaseModel ): CONFIGURATIONS = { \"derived\" : { \"asset\" : \"something.txt\" }} def _load ( self ): ...","title":"Abstract models"},{"location":"library/models/overview/","text":"In modelkit , a Model is simply a subclass of modelkit.Model that implements a _predict function. from modelkit import Model class MyModel ( Model ): def _predict ( self , item ): ... # prediction code goes here ... return result And that's it! With this little boilerplate code, you can now call the model to get predictions, have them batched, or exposed in an API, etc. Instantiating Models \u00b6 Simple models \u00b6 Very simple Model objects can be created and instantiated straight away, that is when they do not load complex assets, or rely on other Model objects for their execution. For example: from modelkit import Model class MyModel ( Model ): def _predict ( self , item ): return item m = MyModel () Complex models \u00b6 In general, however, to resolve assets, dependencies, etc. modelkit models need to be instantiated using ModelLibrary with a set of models. Models are then accessed via ModelLibrary.get(\"some name\") . For example, to load a model that has one dependency: from modelkit import ModelLibrary , Model # Create two models, with names class MyModel ( Model ): CONFIGURATIONS = { \"a_model\" : {} } def _predict ( self , item ): return item class MyComposedModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : { \"model_dependencies\" : { \"a_model\" } } } def _predict ( self , item ): return self . model_dependencies [ \"a_model\" ] . predict ( item ) # Create the model library # This loads the required models (including their dependencies) library = ModelLibrary ( required_models = [ \"my_favorite_model\" ], models = [ MyModel , MyComposedModel ] ) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" ) The library gives you access to all the models from a single object, and deals with instantiation of the necessary objects. Furthermore, modelkit encourages you to store your models in a Python package (see Organization ).","title":"Overview"},{"location":"library/models/overview/#instantiating-models","text":"","title":"Instantiating Models"},{"location":"library/models/overview/#simple-models","text":"Very simple Model objects can be created and instantiated straight away, that is when they do not load complex assets, or rely on other Model objects for their execution. For example: from modelkit import Model class MyModel ( Model ): def _predict ( self , item ): return item m = MyModel ()","title":"Simple models"},{"location":"library/models/overview/#complex-models","text":"In general, however, to resolve assets, dependencies, etc. modelkit models need to be instantiated using ModelLibrary with a set of models. Models are then accessed via ModelLibrary.get(\"some name\") . For example, to load a model that has one dependency: from modelkit import ModelLibrary , Model # Create two models, with names class MyModel ( Model ): CONFIGURATIONS = { \"a_model\" : {} } def _predict ( self , item ): return item class MyComposedModel ( Model ): CONFIGURATIONS = { \"my_favorite_model\" : { \"model_dependencies\" : { \"a_model\" } } } def _predict ( self , item ): return self . model_dependencies [ \"a_model\" ] . predict ( item ) # Create the model library # This loads the required models (including their dependencies) library = ModelLibrary ( required_models = [ \"my_favorite_model\" ], models = [ MyModel , MyComposedModel ] ) # This is only a dictionary lookup model = library . get ( \"my_favorite_model\" ) The library gives you access to all the models from a single object, and deals with instantiation of the necessary objects. Furthermore, modelkit encourages you to store your models in a Python package (see Organization ).","title":"Complex models"},{"location":"library/models/testing/","text":"modelkit provides helper functions to test modelkit.Model instances either directly or with pytest Since test cases constitute essential documentation for developers and as a result should appear close to the code of the model itself, much like the signature. Defining test cases \u00b6 Any modelkit.core.Model can define its own test cases which are discoverable by the test created by make_modellibrary_test . There are two ways of defining test cases, either at the class or the configuration level. At the class level \u00b6 Tests added to the TEST_CASES class attribute are shared across the different models defined in the CONFIGURATIONS map. class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model_a\" : {}, \"some_model_b\" : {}} TEST_CASES = [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ] def _predict ( self , item ): return item In the above example, 4 test cases will be ran: 2 for some_model_a 2 for some_model_b At the configuration level \u00b6 Tests added to the CONFIGURATIONS map are restricted to their parent. In the following example, 2 test cases will be ran for some_model_a : class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model_a\" : { \"test_cases\" : [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ], }, \"some_model_b\" : {}, } def _predict ( self , item ): return item Both ways of testing can be used simultaneously and interchangeably. Running tests \u00b6 The easiest way to carry out test cases in interactive programming (ipython, jupyter notebook etc.) is to use the .test() method inherited from BaseModel. This way, one could easily test the model: from modelkit import Model class NOTModel ( Model ): CONFIGURATIONS = { \"not_model\" : {}} TEST_CASES = [ { \"item\" : True , \"result\" : False }, { \"item\" : False , \"result\" : False } # this should raise an error ] def _predict ( self , item : bool , ** _ ) -> bool : return not item # Execute tests NOTModel () . test () Will return TEST 1 : SUCCESS TEST 2 : FAILED test failed on item item = False expected = False result = True pytest integration \u00b6 ModelLibrary fixture \u00b6 modelkit.core.fixtures.make_modellibrary_test creates a ModelLibrary fixture and a test that can be used in your pytest testing suite. Call the following in a test file discoverable by pytest : from modelkit.core.fixtures import make_modellibrary_test make_modellibrary_test ( ** model_library_arguments , # insert any arguments to ModelLibrary here fixture_name = \"testing_model_library\" , test_name = \"test_auto_model_library\" , ) This will create a pytest fixture called testing_model_library that returns ModelLibrary(**model_library_arguments) which you can freely reuse. Automatic testing \u00b6 In addition, it creates a test called test_auto_model_library that iterates through the tests defined as part of Model classes. Each test is instantiated with an item value and a result value, the automatic test will iterate through them and run the equivalent of: @pytest . mark . parametrize ( \"model_key, item, result\" , [ case for case in Model . TEST_CASES ]) def test_function ( model_key , item , result , testing_model_library ): lib = testing_model_library . getfixturevalue ( fixture_name ) assert lib . get ( model_key )( item ) == result","title":"Testing"},{"location":"library/models/testing/#defining-test-cases","text":"Any modelkit.core.Model can define its own test cases which are discoverable by the test created by make_modellibrary_test . There are two ways of defining test cases, either at the class or the configuration level.","title":"Defining test cases"},{"location":"library/models/testing/#at-the-class-level","text":"Tests added to the TEST_CASES class attribute are shared across the different models defined in the CONFIGURATIONS map. class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model_a\" : {}, \"some_model_b\" : {}} TEST_CASES = [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ] def _predict ( self , item ): return item In the above example, 4 test cases will be ran: 2 for some_model_a 2 for some_model_b","title":"At the class level"},{"location":"library/models/testing/#at-the-configuration-level","text":"Tests added to the CONFIGURATIONS map are restricted to their parent. In the following example, 2 test cases will be ran for some_model_a : class TestableModel ( Model [ ModelItemType , ModelItemType ]): CONFIGURATIONS : Dict [ str , Dict ] = { \"some_model_a\" : { \"test_cases\" : [ { \"item\" : { \"x\" : 1 }, \"result\" : { \"x\" : 1 }}, { \"item\" : { \"x\" : 2 }, \"result\" : { \"x\" : 2 }}, ], }, \"some_model_b\" : {}, } def _predict ( self , item ): return item Both ways of testing can be used simultaneously and interchangeably.","title":"At the configuration level"},{"location":"library/models/testing/#running-tests","text":"The easiest way to carry out test cases in interactive programming (ipython, jupyter notebook etc.) is to use the .test() method inherited from BaseModel. This way, one could easily test the model: from modelkit import Model class NOTModel ( Model ): CONFIGURATIONS = { \"not_model\" : {}} TEST_CASES = [ { \"item\" : True , \"result\" : False }, { \"item\" : False , \"result\" : False } # this should raise an error ] def _predict ( self , item : bool , ** _ ) -> bool : return not item # Execute tests NOTModel () . test () Will return TEST 1 : SUCCESS TEST 2 : FAILED test failed on item item = False expected = False result = True","title":"Running tests"},{"location":"library/models/testing/#pytest-integration","text":"","title":"pytest integration"},{"location":"library/models/testing/#modellibrary-fixture","text":"modelkit.core.fixtures.make_modellibrary_test creates a ModelLibrary fixture and a test that can be used in your pytest testing suite. Call the following in a test file discoverable by pytest : from modelkit.core.fixtures import make_modellibrary_test make_modellibrary_test ( ** model_library_arguments , # insert any arguments to ModelLibrary here fixture_name = \"testing_model_library\" , test_name = \"test_auto_model_library\" , ) This will create a pytest fixture called testing_model_library that returns ModelLibrary(**model_library_arguments) which you can freely reuse.","title":"ModelLibrary fixture"},{"location":"library/models/testing/#automatic-testing","text":"In addition, it creates a test called test_auto_model_library that iterates through the tests defined as part of Model classes. Each test is instantiated with an item value and a result value, the automatic test will iterate through them and run the equivalent of: @pytest . mark . parametrize ( \"model_key, item, result\" , [ case for case in Model . TEST_CASES ]) def test_function ( model_key , item , result , testing_model_library ): lib = testing_model_library . getfixturevalue ( fixture_name ) assert lib . get ( model_key )( item ) == result","title":"Automatic testing"},{"location":"library/models/using_models/","text":"Using modelkit models \u00b6 The first thing you will want to do with a Model is get predictions from it. There are three ways to do so. Let us consider this example, in which we have implemented _predict class MyModel ( Model ): def _predict ( self , item , ** kwargs ): return item , kwargs Predictions for single items \u00b6 Predictions for a single item can be obtained by calling the object, or it's predict function. prediction = model ( item ) # or model.predict(item) # returns item This will call whichever one of _predict or _predict_batch was implemented in the Model . Note Although you have implemented _predict , you need to call predict (no underscore) to get predictions. The difference between the two is where modelkit 's magic operates \ud83d\ude00 Note that keyword arguments will be passed all the way to the implemented _predict : prediction = model ( item , some_kwarg = 10 ) # or model.predict(item) # returns item, {\"some_kwargs\": 10} Predictions for lists of items \u00b6 Predictions for list of items can also easily be obtained by using predict_batch : items = [ 1 , 2 , 3 , 4 ] predictions = model . predict_batch ( items ) Here, items must be a list of items. modelkit will iterate on it and fetch predictions. This will also call whichever one of _predict or _predict_batch was implemented in the Model , and pass kwargs . Predictions from iterators \u00b6 It is also possible to iterate through predictions with an iterator, which is convenient to avoid having to load all items to memory before getting predictions. def generate_items (): ... yield item for prediction in model . predict_gen ( generate_items ()): # use prediction ... A typical use case is to iterate through the lines of a file, perform some processing and write it straight back to another file Note that in the case in which _predict_batch is implemented, you may see speed ups if you have written vectorized code. See the documentation on batching for more information","title":"Getting predictions"},{"location":"library/models/using_models/#using-modelkit-models","text":"The first thing you will want to do with a Model is get predictions from it. There are three ways to do so. Let us consider this example, in which we have implemented _predict class MyModel ( Model ): def _predict ( self , item , ** kwargs ): return item , kwargs","title":"Using modelkit models"},{"location":"library/models/using_models/#predictions-for-single-items","text":"Predictions for a single item can be obtained by calling the object, or it's predict function. prediction = model ( item ) # or model.predict(item) # returns item This will call whichever one of _predict or _predict_batch was implemented in the Model . Note Although you have implemented _predict , you need to call predict (no underscore) to get predictions. The difference between the two is where modelkit 's magic operates \ud83d\ude00 Note that keyword arguments will be passed all the way to the implemented _predict : prediction = model ( item , some_kwarg = 10 ) # or model.predict(item) # returns item, {\"some_kwargs\": 10}","title":"Predictions for single items"},{"location":"library/models/using_models/#predictions-for-lists-of-items","text":"Predictions for list of items can also easily be obtained by using predict_batch : items = [ 1 , 2 , 3 , 4 ] predictions = model . predict_batch ( items ) Here, items must be a list of items. modelkit will iterate on it and fetch predictions. This will also call whichever one of _predict or _predict_batch was implemented in the Model , and pass kwargs .","title":"Predictions for lists of items"},{"location":"library/models/using_models/#predictions-from-iterators","text":"It is also possible to iterate through predictions with an iterator, which is convenient to avoid having to load all items to memory before getting predictions. def generate_items (): ... yield item for prediction in model . predict_gen ( generate_items ()): # use prediction ... A typical use case is to iterate through the lines of a file, perform some processing and write it straight back to another file Note that in the case in which _predict_batch is implemented, you may see speed ups if you have written vectorized code. See the documentation on batching for more information","title":"Predictions from iterators"},{"location":"library/models/validation/","text":"Model typing \u00b6 It is also possible to provide types for a Model subclass, such that linters and callers know exactly which item type is expected, and what the result of a Model call looks like. Types are specified when instantiating the Model class: # This model takes `str` items and always returns `int` values class SomeTypedModel ( Model [ str , int ]): def _predict ( self , item ): return len ( item ) Static type checking \u00b6 Setting Model types allows static type checkers to fail if the expected return value of calls to predict have the wrong types. Consider the above model: m = SomeTypedModel () x : int = m ( \"ok\" ) y : List [ int ] = m ([ \"ok\" , \"boomer\" ]) z : int = m ( 1 ) # would lead to a typing error with typecheckers (e.g. mypy) Runtime type validation \u00b6 In addition, whenever the model's predict method is called, the type of the item is validated against the provided type and raises an error if the validation fails: modelkit.core.model.ItemValidationException if the item fails to validate modelkit.core.model.ReturnValueValidationException if the return value of the predict fails to validate Marshalling of item/return values \u00b6 It is possible to specify a pydantic.BaseModel subtype as a type argument for Model classes. This will actually change the structure of the data that is fed into to the _predict method. For example: class ItemModel ( pydantic . BaseModel ): x : int class ReturnModel ( pydantic . BaseModel ): x : int class SomeValidatedModel ( Model [ ItemModel , ReturnModel ]): def _predict ( self , item ): # item is guaranteed to be an instance of `ItemModel` even if we feed a dictionary item result = { \"x\" : item . x } # We can either return a dictionary return result # or return the pydantic structure # return ReturnModel(x = item.x) m = SomeValidatedModel () # although we return a dict from the _predict method, return value # is turned into a `ReturnModel` instance. y : ReturnModel = m ({ \"x\" : 1 }) This also works with list of items class SomeValidatedModelBatch ( Model [ ItemModel , ReturnModel ]): def _predict_batch ( self , items ): return [{ \"x\" : item . x } for item in items ] m = SomeValidatedModelBatch () y : List [ ReturnModel ] = m . predict_batch ( items = [{ \"x\" : 1 }, { \"x\" : 2 }]) Note Note that, although we call predict with a dictionary, _predict will see pydantic structures. Importantly, this means that attributes now need to be refered to with natural naming : item.x instead of item[\"x\"] Disabling validation \u00b6 pydantic validation can take some time, and in some cases the validation may end up taking much more time than the prediction itself. This occurs generally when: a Model 's payload is large (contains long lists of objects to validate) a Model 's prediction is very simple To avoid the validation overhead, especially in production scenarios, it is possible to ask modelkit to create models without validation , which will be faster in general. This also still creates pydantic structure and therefore will now break the natural naming inside the predict function.","title":"Validation"},{"location":"library/models/validation/#model-typing","text":"It is also possible to provide types for a Model subclass, such that linters and callers know exactly which item type is expected, and what the result of a Model call looks like. Types are specified when instantiating the Model class: # This model takes `str` items and always returns `int` values class SomeTypedModel ( Model [ str , int ]): def _predict ( self , item ): return len ( item )","title":"Model typing"},{"location":"library/models/validation/#static-type-checking","text":"Setting Model types allows static type checkers to fail if the expected return value of calls to predict have the wrong types. Consider the above model: m = SomeTypedModel () x : int = m ( \"ok\" ) y : List [ int ] = m ([ \"ok\" , \"boomer\" ]) z : int = m ( 1 ) # would lead to a typing error with typecheckers (e.g. mypy)","title":"Static type checking"},{"location":"library/models/validation/#runtime-type-validation","text":"In addition, whenever the model's predict method is called, the type of the item is validated against the provided type and raises an error if the validation fails: modelkit.core.model.ItemValidationException if the item fails to validate modelkit.core.model.ReturnValueValidationException if the return value of the predict fails to validate","title":"Runtime type validation"},{"location":"library/models/validation/#marshalling-of-itemreturn-values","text":"It is possible to specify a pydantic.BaseModel subtype as a type argument for Model classes. This will actually change the structure of the data that is fed into to the _predict method. For example: class ItemModel ( pydantic . BaseModel ): x : int class ReturnModel ( pydantic . BaseModel ): x : int class SomeValidatedModel ( Model [ ItemModel , ReturnModel ]): def _predict ( self , item ): # item is guaranteed to be an instance of `ItemModel` even if we feed a dictionary item result = { \"x\" : item . x } # We can either return a dictionary return result # or return the pydantic structure # return ReturnModel(x = item.x) m = SomeValidatedModel () # although we return a dict from the _predict method, return value # is turned into a `ReturnModel` instance. y : ReturnModel = m ({ \"x\" : 1 }) This also works with list of items class SomeValidatedModelBatch ( Model [ ItemModel , ReturnModel ]): def _predict_batch ( self , items ): return [{ \"x\" : item . x } for item in items ] m = SomeValidatedModelBatch () y : List [ ReturnModel ] = m . predict_batch ( items = [{ \"x\" : 1 }, { \"x\" : 2 }]) Note Note that, although we call predict with a dictionary, _predict will see pydantic structures. Importantly, this means that attributes now need to be refered to with natural naming : item.x instead of item[\"x\"]","title":"Marshalling of item/return values"},{"location":"library/models/validation/#disabling-validation","text":"pydantic validation can take some time, and in some cases the validation may end up taking much more time than the prediction itself. This occurs generally when: a Model 's payload is large (contains long lists of objects to validate) a Model 's prediction is very simple To avoid the validation overhead, especially in production scenarios, it is possible to ask modelkit to create models without validation , which will be faster in general. This also still creates pydantic structure and therefore will now break the natural naming inside the predict function.","title":"Disabling validation"},{"location":"library/special/distant/","text":"DistantHTTPModel \u00b6 Sometimes models will simply need to call another microservice, in this case DistantHTTPModel are the way to go. They are instantiated with a POST endpoint URL. from modelkit.core.models.distant_model import DistantHTTPModel class SomeDistantHTTPModel ( DistantHTTPModel ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"endpoint\" : \"http://127.0.0.1:8000/api/path/endpoint\" , } } } When predict is called, a request is made to the http://127.0.0.1:8000/api/path/endpoint with the complete input item serialized in the body and the response of the server is returned. In addition, it is possible to set this behavior at the level of the ModelLibrary by either setting the async_mode setting in the LibrarySettings or by setting the environment variable modelkit_ASYNC_MODE . async support \u00b6 The AsyncDistantHTTPModel provides a base class with the same interface as DistantHTTPModel but supports distant requests with aiohttp . Closing connections \u00b6 To close connections, you can do it at the level of the ModelLibrary either calling: ModelLibrary.close() in a synchronous context await ModelLibrary.aclose() in an asynchronous context This will iterate through all existing models and call close (which is either sync or async according to the model type).","title":"Distant Models"},{"location":"library/special/distant/#distanthttpmodel","text":"Sometimes models will simply need to call another microservice, in this case DistantHTTPModel are the way to go. They are instantiated with a POST endpoint URL. from modelkit.core.models.distant_model import DistantHTTPModel class SomeDistantHTTPModel ( DistantHTTPModel ): CONFIGURATIONS = { \"some_model\" : { \"model_settings\" : { \"endpoint\" : \"http://127.0.0.1:8000/api/path/endpoint\" , } } } When predict is called, a request is made to the http://127.0.0.1:8000/api/path/endpoint with the complete input item serialized in the body and the response of the server is returned. In addition, it is possible to set this behavior at the level of the ModelLibrary by either setting the async_mode setting in the LibrarySettings or by setting the environment variable modelkit_ASYNC_MODE .","title":"DistantHTTPModel"},{"location":"library/special/distant/#async-support","text":"The AsyncDistantHTTPModel provides a base class with the same interface as DistantHTTPModel but supports distant requests with aiohttp .","title":"async support"},{"location":"library/special/distant/#closing-connections","text":"To close connections, you can do it at the level of the ModelLibrary either calling: ModelLibrary.close() in a synchronous context await ModelLibrary.aclose() in an asynchronous context This will iterate through all existing models and call close (which is either sync or async according to the model type).","title":"Closing connections"},{"location":"library/special/tensorflow/","text":"Tensorflow models \u00b6 modelkit provides different modes to use TF models, and makes it easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC TensorflowModel class \u00b6 All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_dtypes a dict of shapes and dtypes of these tensors. Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray . Other convenience methods \u00b6 Post processing \u00b6 After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features. Empty predictions \u00b6 Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial, in which case we need not bother calling TF with anything. modelkit provides a built-in mechanism do deal with these \"empty\" examples, and the default implementation of predict_batch uses it. To make use of it, override the _is_empty method: def _is_empty ( self , item ) -> bool : return item == \"\" This will fill in missing values with zeroed arrays when empty strings are found, without calling TF. To fill in values with another array, also override the _generate_empty_prediction method def _generate_empty_prediction ( self ) -> Dict [ str , Any ]: \"\"\"Function used to fill in values when rebuilding predictions with the mask\"\"\" return { name : np . zeros (( 1 ,) + self . output_shapes [ name ], self . output_dtypes [ name ]) for name in self . output_tensor_mapping } TF Serving \u00b6 modelkit provides an easy way to query Tensorflow models served via TF Serving. When TF serving is configured, the TF models are not run in the main process, but queried. Running TF serving container locally \u00b6 In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by modelkit tf-serving local-docker --models [ PACKAGE ] The CLI creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR is bound to the /config directory inside the container. Specifically, the CLI: Instantiates a ModelLibrary with all configured models in PACKAGE Downloads all necessary assets in the MODELKIT_ASSETS_DIR writes a configuration file under the local MODELKIT_ASSETS_DIR with all TF models that are configured The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/config.config : docker run \\ --name local-tf-serving \\ -d \\ -p 8500 :8500 -p 8501 :8501 \\ -v ${ MODELKIT_ASSETS_DIR } :/config \\ -t tensorflow/serving \\ --model_config_file = /config/config.config \\ --rest_api_port = 8501 \\ --port = 8500 See also: the CLI documentation . the Tensorflow serving documentation the Tensorflow serving github Internal TF serving settings \u00b6 Several environment variables control how modelkit requests predictions from TF serving. MODELKIT_TF_SERVING_ENABLE : Controls whether to use TF serving or use TF locally as a lib MODELKIT_TF_SERVING_HOST : Host to connect to to request TF predictions MODELKIT_TF_SERVING_PORT : Port to connect to to request TF predictions MODELKIT_TF_SERVING_MODE : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) MODELKIT_TF_SERVING_ATTEMPTS : number of attempts to wait for TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings): lib_serving_grpc = ModelLibrary ( required_models =... , settings = LibrarySettings ( tf_serving = { \"enable\" : True , \"port\" : 8500 , \"mode\" : \"grpc\" , \"host\" : \"localhost\" , } ), models =... , ) Using TF Serving during tests \u00b6 modelkit provides a fixture to run TF serving during testing: @pytest . fixture ( scope = \"session\" ) def tf_serving (): lib = ModelLibrary ( models =... , settings = { \"lazy_loading\" : True }) yield tf_serving_fixture ( request , lib ) This will configure and run TF serving during the test session, provided docker is present.","title":"Tensorflow models"},{"location":"library/special/tensorflow/#tensorflow-models","text":"modelkit provides different modes to use TF models, and makes it easy to switch between them. calling the TF model using the tensorflow module requesting predictions from TensorFlow Serving synchronously via a REST API requesting predictions from TensorFlow Serving asynchronously via a REST API requesting predictions from TensorFlow Serving synchronously via gRPC","title":"Tensorflow models"},{"location":"library/special/tensorflow/#tensorflowmodel-class","text":"All tensorflow based models should derive from the TensorflowModel class. This class provides a number of functions that help with loading/serving TF models. At initialization time, a TensorflowModel has to be provided with definitions of the tensors predicted by the TF model: output_tensor_mapping a dict of arbitrary key s to tensor names describing the outputs. output_tensor_shapes and output_dtypes a dict of shapes and dtypes of these tensors. Important Be careful that _tensorflow_predict returns a dict of np.ndarray of shape (len(items),?) when _predict_batch expects a list of len(items) dicts of np.ndarray .","title":"TensorflowModel class"},{"location":"library/special/tensorflow/#other-convenience-methods","text":"","title":"Other convenience methods"},{"location":"library/special/tensorflow/#post-processing","text":"After the TF call, _tensorflow_predict_* returns a dict of np.ndarray of shape (len(items),?) . These can be further manipulated by reimplementing the TensorflowModel._post_processing function, e.g. to reshape, change type, select a subset of features.","title":"Post processing"},{"location":"library/special/tensorflow/#empty-predictions","text":"Oftentimes we manipulate the item before feeding it to TF, e.g. doing text cleaning or vectorization. This sometimes results in making the prediction trivial, in which case we need not bother calling TF with anything. modelkit provides a built-in mechanism do deal with these \"empty\" examples, and the default implementation of predict_batch uses it. To make use of it, override the _is_empty method: def _is_empty ( self , item ) -> bool : return item == \"\" This will fill in missing values with zeroed arrays when empty strings are found, without calling TF. To fill in values with another array, also override the _generate_empty_prediction method def _generate_empty_prediction ( self ) -> Dict [ str , Any ]: \"\"\"Function used to fill in values when rebuilding predictions with the mask\"\"\" return { name : np . zeros (( 1 ,) + self . output_shapes [ name ], self . output_dtypes [ name ]) for name in self . output_tensor_mapping }","title":"Empty predictions"},{"location":"library/special/tensorflow/#tf-serving","text":"modelkit provides an easy way to query Tensorflow models served via TF Serving. When TF serving is configured, the TF models are not run in the main process, but queried.","title":"TF Serving"},{"location":"library/special/tensorflow/#running-tf-serving-container-locally","text":"In order to run a TF Serving docker locally, one first needs to download the models and write a configuration file. This can be achieved by modelkit tf-serving local-docker --models [ PACKAGE ] The CLI creates a configuration file for tensorflow serving, with the model locations refered to relative to the container file system . As a result, the TF serving container will expect that the MODELKIT_ASSETS_DIR is bound to the /config directory inside the container. Specifically, the CLI: Instantiates a ModelLibrary with all configured models in PACKAGE Downloads all necessary assets in the MODELKIT_ASSETS_DIR writes a configuration file under the local MODELKIT_ASSETS_DIR with all TF models that are configured The container can then be started by pointing TF serving to the generated configuration file --model_config_file=/config/config.config : docker run \\ --name local-tf-serving \\ -d \\ -p 8500 :8500 -p 8501 :8501 \\ -v ${ MODELKIT_ASSETS_DIR } :/config \\ -t tensorflow/serving \\ --model_config_file = /config/config.config \\ --rest_api_port = 8501 \\ --port = 8500 See also: the CLI documentation . the Tensorflow serving documentation the Tensorflow serving github","title":"Running TF serving container locally"},{"location":"library/special/tensorflow/#internal-tf-serving-settings","text":"Several environment variables control how modelkit requests predictions from TF serving. MODELKIT_TF_SERVING_ENABLE : Controls whether to use TF serving or use TF locally as a lib MODELKIT_TF_SERVING_HOST : Host to connect to to request TF predictions MODELKIT_TF_SERVING_PORT : Port to connect to to request TF predictions MODELKIT_TF_SERVING_MODE : Can be grpc (with grpc ) or rest (with requests for TensorflowModel , or with aiohttp for AsyncTensorflowModel ) MODELKIT_TF_SERVING_ATTEMPTS : number of attempts to wait for TF serving response All of these parameters can be set programmatically (and passed to the ModelLibrary 's settings): lib_serving_grpc = ModelLibrary ( required_models =... , settings = LibrarySettings ( tf_serving = { \"enable\" : True , \"port\" : 8500 , \"mode\" : \"grpc\" , \"host\" : \"localhost\" , } ), models =... , )","title":"Internal TF serving settings"},{"location":"library/special/tensorflow/#using-tf-serving-during-tests","text":"modelkit provides a fixture to run TF serving during testing: @pytest . fixture ( scope = \"session\" ) def tf_serving (): lib = ModelLibrary ( models =... , settings = { \"lazy_loading\" : True }) yield tf_serving_fixture ( request , lib ) This will configure and run TF serving during the test session, provided docker is present.","title":"Using TF Serving during tests"}]}